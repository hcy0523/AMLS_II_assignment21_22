{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d60291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load main.py\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import nltk\n",
    "import ekphrasis\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1415c",
   "metadata": {},
   "source": [
    "### Step1: Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f8f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "Path =os.path.dirname(os.getcwd())\n",
    "data_pathA=os.path.join(Path,'Datasets/A/twitter-2016train-A.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a8db65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "      <th>Nan</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619950566786113536</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Picturehouse's, Pink Floyd's, 'Roger Waters: T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>619969366986235905</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Order Go Set a Watchman in store or through ou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>619971047195045888</td>\n",
       "      <td>negative</td>\n",
       "      <td>If these runway renovations at the airport pre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>619974445185302528</td>\n",
       "      <td>neutral</td>\n",
       "      <td>If you could ask an onstage interview question...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>619987808317407232</td>\n",
       "      <td>positive</td>\n",
       "      <td>A portion of book sales from our Harper Lee/Go...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20627</th>\n",
       "      <td>681877834982232064</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@ShaquilleHoNeal from what I think you're aski...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20628</th>\n",
       "      <td>681879579129200640</td>\n",
       "      <td>positive</td>\n",
       "      <td>Iran ranks 1st in liver surgeries, Allah bless...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20629</th>\n",
       "      <td>681883903259357184</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Hours before he arrived in Saudi Arabia on Tue...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20630</th>\n",
       "      <td>681904976860327936</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VanityFair  Alex Kim Kardashian worth how to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20631</th>\n",
       "      <td>681910549211287552</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I guess even Pandora knows Justin Bieber is a ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20632 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ID Sentiment  \\\n",
       "0      619950566786113536   neutral   \n",
       "1      619969366986235905   neutral   \n",
       "2      619971047195045888  negative   \n",
       "3      619974445185302528   neutral   \n",
       "4      619987808317407232  positive   \n",
       "...                   ...       ...   \n",
       "20627  681877834982232064   neutral   \n",
       "20628  681879579129200640  positive   \n",
       "20629  681883903259357184   neutral   \n",
       "20630  681904976860327936  negative   \n",
       "20631  681910549211287552   neutral   \n",
       "\n",
       "                                                    Text  Nan  label  \n",
       "0      Picturehouse's, Pink Floyd's, 'Roger Waters: T...  NaN      1  \n",
       "1      Order Go Set a Watchman in store or through ou...  NaN      1  \n",
       "2      If these runway renovations at the airport pre...  NaN      0  \n",
       "3      If you could ask an onstage interview question...  NaN      1  \n",
       "4      A portion of book sales from our Harper Lee/Go...  NaN      2  \n",
       "...                                                  ...  ...    ...  \n",
       "20627  @ShaquilleHoNeal from what I think you're aski...  NaN      1  \n",
       "20628  Iran ranks 1st in liver surgeries, Allah bless...  NaN      2  \n",
       "20629  Hours before he arrived in Saudi Arabia on Tue...  NaN      1  \n",
       "20630  @VanityFair  Alex Kim Kardashian worth how to ...  NaN      0  \n",
       "20631  I guess even Pandora knows Justin Bieber is a ...  NaN      1  \n",
       "\n",
       "[20632 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform data into df form\n",
    "dataA = pd.read_table(data_pathA,sep='\\t',header=None)\n",
    "dataA.columns = ['ID','Sentiment','Text','Nan']\n",
    "def add_label(sentiment):\n",
    "    if sentiment == 'negative':\n",
    "        return 0\n",
    "    elif sentiment == 'neutral':\n",
    "        return 1\n",
    "    elif sentiment == 'positive':\n",
    "        return 2\n",
    "\n",
    "dataA['label'] = dataA.Sentiment.apply(add_label)\n",
    "dataA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d65b7d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    10342\n",
       "2     7059\n",
       "0     3231\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment distribution of data\n",
    "dataA.loc[:,'label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443edeb7",
   "metadata": {},
   "source": [
    "1Case conversion\n",
    "包含“India”和“india”的语料库如果不应用小写化，机器会把它们识别为两个独立的术语，而实际上它们都是同一个单词的不同形式，并且对应于同一个国家。小写化后，仅存在一种“India”实例，即“india”，简化了在语料库中找到所有提到印度时的任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24103376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCY\\AppData\\Roaming\\Python\\Python38\\site-packages\\ekphrasis\\classes\\tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCY\\AppData\\Roaming\\Python\\Python38\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "#import ekphrasis library\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcbb4e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HCY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b296cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(Texts):\n",
    "    token=[]\n",
    "    for Text in Texts:\n",
    "        words = [sentence for sentence in text_processor.pre_process_doc(Text) if (sentence!='s' and sentence!='\\'')]\n",
    "        words = [word for word in words if (word not in stop)]\n",
    "        token.append(words)\n",
    "    words=[word for words in token for word in words]\n",
    "    \n",
    "    print(\"All words: {}\".format(len(words)))\n",
    "    # Create Counter\n",
    "    counts = Counter(words)\n",
    "    print(\"Unique words: {}\".format(len(counts)))\n",
    "\n",
    "    Most_common= counts.most_common()[:30]\n",
    "    print(\"Top 30 most common words: {}\".format(Most_common))\n",
    "    \n",
    "    vocab = {word: num for num, word in enumerate(counts, 1)}\n",
    "    id2vocab = {v: k for k, v in vocab.items()}\n",
    "    return token,vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a39875da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All words: 333435\n",
      "Unique words: 21914\n",
      "Top 30 most common words: [('.', 18786), (',', 9728), ('<user>', 7027), ('<url>', 6377), ('<number>', 5262), ('<repeated>', 5142), ('<hashtag>', 5092), ('</hashtag>', 5092), ('!', 4391), ('<allcaps>', 4306), ('</allcaps>', 4306), ('may', 3563), ('-', 3550), ('tomorrow', 2953), ('?', 2787), ('\"', 2731), (':', 2682), ('th', 2161), ('1', 2106), ('day', 1714), ('<date>', 1701), ('2', 1379), ('st', 1336), ('friday', 1328), ('see', 1305), ('sunday', 1285), ('&', 1282), ('night', 1267), ('like', 1207), ('going', 1162)]\n"
     ]
    }
   ],
   "source": [
    "token,vocab=Tokenize(dataA.Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ca1c7",
   "metadata": {},
   "source": [
    "### Step2: Word2Vec Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e51b0eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk import word_tokenize\n",
    "import multiprocessing\n",
    "import tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcf25f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model=Word2Vec(token,window=5, min_count=1,workers = multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b4c56bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26101584, 33343500)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.train(token, total_examples = len(token), epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b864413b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is summary of Word2Vec: Word2Vec(vocab=21914, vector_size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print('This is summary of Word2Vec: {}'.format(word2vec_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f349892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=word2vec_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ada745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.save_word2vec_format('Word2Vec.vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e06de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_path=os.path.join(Path,'Datasets/datastories.twitter.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f820ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_matrix = np.zeros((len(index), 100))\n",
    "embed_dict={}\n",
    "for word, i in index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embed_matrix[i] = word2vec_model.wv[word]\n",
    "        embed_dict[word] = word2vec_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c0514fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del word2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d10aa858",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = pd.read_table(word_path,sep=' ',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c371b3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "word.set_index(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f5bb152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Embed_dict={}\n",
    "for i in range(word.shape[0]):\n",
    "    Embed_dict[word.index[i]]=word.iloc[i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eed71ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed_path=os.path.join(Path,'Datasets/Embed_dict')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ded29",
   "metadata": {},
   "source": [
    "### Step3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4e8d7c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCY\\AppData\\Roaming\\Python\\Python38\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.6.0\n",
      "Eager mode:  True\n",
      "GPU is available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCY\\AppData\\Local\\Temp\\ipykernel_6440\\1929975368.py:22: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  display.set_matplotlib_formats('svg')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, SpatialDropout1D, Bidirectional\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import transformers\n",
    "import tensorflow_hub as hub\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "display.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32cc1d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_lstm(X, vocab, seq_len):\n",
    "    '''\n",
    "    Returns tokenized tensor with left/right padding at the specified sequence length\n",
    "    '''\n",
    "    X_tmp = np.zeros((len(X), seq_len), dtype=np.int64)\n",
    "    for i, text in enumerate(X):\n",
    "        tokens = [word for word in text_processor.pre_process_doc(text) if (word!='s' and word!='\\'')]\n",
    "        tokens = [word for word in tokens if (word not in stop)]\n",
    "        token_ids = [vocab[word] for word in tokens if word in embed_dict.keys()]###\n",
    "        end_idx = min(len(token_ids), seq_len)\n",
    "        start_idx = max(seq_len - len(token_ids), 0)\n",
    "        X_tmp[i,start_idx:] = token_ids[:end_idx]\n",
    "\n",
    "    return X_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74d4ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tokenizer_lstm(dataA.Text, vocab, 100)###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbd63b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1654"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['rock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53a0b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.one_hot(dataA.label, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93237066",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y= np.array(Y)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size=0.1, random_state=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "823b8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_layer(vocab, embeddings_index):\n",
    "    \"\"\"\n",
    "    Build embedding matrix and embedding layer\n",
    "    :param vocab_size: vocabulary size\n",
    "    :param tok: tokenizer\n",
    "    :param embeddings_index: embedding index\n",
    "    :return: embedding matrix and embedding layer\n",
    "    \"\"\"\n",
    "    #Build embedding matrix\n",
    "    vocab_size=len(vocab)+1\n",
    "    embedding_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, i in vocab.items():\n",
    "        # Vector corresponds to word\n",
    "        embedding_vector = embed_dict.get(word)###,embed_dict['<unk>']\n",
    "\n",
    "        if embedding_vector is not None:\n",
    "            # Ensure vector of embedding_matrix row matches word index\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    # Build embedding layer\n",
    "    embedding_layer = Embedding(input_dim = vocab_size, output_dim = 100, weights = [embedding_matrix], input_length = 100, trainable=False)\n",
    "    return embedding_layer,embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "682b9f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_embedding_layer(vocab, embeddings_index):\n",
    "#     \"\"\"\n",
    "#     Build embedding matrix and embedding layer\n",
    "#     :param vocab_size: vocabulary size\n",
    "#     :param tok: tokenizer\n",
    "#     :param embeddings_index: embedding index\n",
    "#     :return: embedding matrix and embedding layer\n",
    "#     \"\"\"\n",
    "#     #Build embedding matrix\n",
    "#     vocab_size=len(vocab)+1\n",
    "#     embedding_matrix = np.zeros((vocab_size, 100))\n",
    "#     for word, i in vocab.items():\n",
    "#         try:\n",
    "#             # Vector corresponds to word\n",
    "#             embedding_vector = embeddings_index.get(word)\n",
    "#         except:\n",
    "#             embedding_vector = embeddings_index['<unk>']#['unknown']#['<unk>']\n",
    "#         if embedding_vector is not None:\n",
    "#             # Ensure vector of embedding_matrix row matches word index\n",
    "#             embedding_matrix[i] = embedding_vector\n",
    "#     # Build embedding layer\n",
    "#     embedding_layer = Embedding(input_dim = vocab_size, output_dim = 100, weights = [embedding_matrix], input_length = 100, trainable=False)\n",
    "#     return embedding_layer,embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a0258fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21914"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62dfdb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer,embedding_matrix=build_embedding_layer(vocab, embed_dict) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41d66a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21915"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffabdbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "del Embed_dict,embedding_matrix,embed_dict,embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f34afba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(X_train, y_train, embedding_layer):\n",
    "        \"\"\"\n",
    "        Train, validate and test BiLSTM model, calculate accuracy of training and validation set\n",
    "        :param X_train: tweet train data\n",
    "        :param y_train: sentiment label train data\n",
    "        :param embedding_layer: embedding layer\n",
    "        :param X_test: tweet test data\n",
    "        :param y_test: sentiment label test data\n",
    "        :return: accuracy, recall, precision, F1 score and history\n",
    "        \"\"\"\n",
    "        tf.debugging.set_log_device_placement(True)\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        model.add(SpatialDropout1D(0.2))\n",
    "        \n",
    "#         LSTM(128, dropout = 0.2, recurrent_dropout = 0.5)\n",
    "\n",
    "#         LSTM(128,activation='tanh', recurrent_activation='sigmoid',\n",
    "#              use_bias=True,dropout=0.5,recurrent_dropout=0.0)\n",
    "    \n",
    "#         model.add(Bidirectional(LSTM(128,dropout = 0.5,return_sequences=True)))\n",
    "        model.add(Bidirectional(LSTM(128,dropout = 0.2,recurrent_dropout = 0.5)))\n",
    "#         model.add(Bidirectional(LSTM(128,dropout = 0.2,recurrent_dropout = 0.5)))\n",
    "        \n",
    "        model.add(Dense(3, activation = 'softmax'))\n",
    "        model.summary()\n",
    "        model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "        history = model.fit(X_train, y_train, validation_split = 0.2, epochs = 26, batch_size = 256)\n",
    "        model.save('taskA.h5')\n",
    "        train_acc = history.history['accuracy'][-1]\n",
    "        val_acc = history.history['val_accuracy'][-1]\n",
    "        return train_acc, val_acc, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c45cbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          2191500   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 2,426,767\n",
      "Trainable params: 235,267\n",
      "Non-trainable params: 2,191,500\n",
      "_________________________________________________________________\n",
      "Epoch 1/26\n",
      "59/59 [==============================] - 80s 1s/step - loss: 0.9300 - accuracy: 0.5444 - val_loss: 0.8282 - val_accuracy: 0.6155\n",
      "Epoch 2/26\n",
      "59/59 [==============================] - 75s 1s/step - loss: 0.8499 - accuracy: 0.6008 - val_loss: 0.8046 - val_accuracy: 0.6292\n",
      "Epoch 3/26\n",
      "59/59 [==============================] - 76s 1s/step - loss: 0.8303 - accuracy: 0.6121 - val_loss: 0.7927 - val_accuracy: 0.6344\n",
      "Epoch 4/26\n",
      "59/59 [==============================] - 78s 1s/step - loss: 0.8044 - accuracy: 0.6276 - val_loss: 0.7765 - val_accuracy: 0.6446\n",
      "Epoch 5/26\n",
      "59/59 [==============================] - 78s 1s/step - loss: 0.7911 - accuracy: 0.6400 - val_loss: 0.7761 - val_accuracy: 0.6546\n",
      "Epoch 6/26\n",
      "59/59 [==============================] - 77s 1s/step - loss: 0.7775 - accuracy: 0.6453 - val_loss: 0.7795 - val_accuracy: 0.6524\n",
      "Epoch 7/26\n",
      "59/59 [==============================] - 75s 1s/step - loss: 0.7767 - accuracy: 0.6472 - val_loss: 0.7775 - val_accuracy: 0.6459\n",
      "Epoch 8/26\n",
      "59/59 [==============================] - 76s 1s/step - loss: 0.7631 - accuracy: 0.6528 - val_loss: 0.7790 - val_accuracy: 0.6449\n",
      "Epoch 9/26\n",
      "59/59 [==============================] - 75s 1s/step - loss: 0.7467 - accuracy: 0.6635 - val_loss: 0.7713 - val_accuracy: 0.6546\n",
      "Epoch 10/26\n",
      "59/59 [==============================] - 76s 1s/step - loss: 0.7427 - accuracy: 0.6662 - val_loss: 0.7756 - val_accuracy: 0.6489\n",
      "Epoch 11/26\n",
      "59/59 [==============================] - 76s 1s/step - loss: 0.7350 - accuracy: 0.6751 - val_loss: 0.7801 - val_accuracy: 0.6516\n",
      "Epoch 12/26\n",
      "59/59 [==============================] - 82s 1s/step - loss: 0.7301 - accuracy: 0.6705 - val_loss: 0.7719 - val_accuracy: 0.6521\n",
      "Epoch 13/26\n",
      "59/59 [==============================] - 76s 1s/step - loss: 0.7158 - accuracy: 0.6781 - val_loss: 0.7778 - val_accuracy: 0.6581\n",
      "Epoch 14/26\n",
      "59/59 [==============================] - 76s 1s/step - loss: 0.7081 - accuracy: 0.6905 - val_loss: 0.7759 - val_accuracy: 0.6546\n",
      "Epoch 15/26\n",
      "59/59 [==============================] - 75s 1s/step - loss: 0.6993 - accuracy: 0.6896 - val_loss: 0.7857 - val_accuracy: 0.6535\n",
      "Epoch 16/26\n",
      "59/59 [==============================] - 76s 1s/step - loss: 0.6932 - accuracy: 0.6947 - val_loss: 0.7898 - val_accuracy: 0.6602\n",
      "Epoch 17/26\n",
      "59/59 [==============================] - 75s 1s/step - loss: 0.6841 - accuracy: 0.6976 - val_loss: 0.7817 - val_accuracy: 0.6581\n",
      "Epoch 18/26\n",
      "59/59 [==============================] - 76s 1s/step - loss: 0.6809 - accuracy: 0.7021 - val_loss: 0.7979 - val_accuracy: 0.6594\n",
      "Epoch 19/26\n",
      "59/59 [==============================] - 77s 1s/step - loss: 0.6712 - accuracy: 0.7059 - val_loss: 0.7907 - val_accuracy: 0.6572\n",
      "Epoch 20/26\n",
      "59/59 [==============================] - 96s 2s/step - loss: 0.6653 - accuracy: 0.7155 - val_loss: 0.8101 - val_accuracy: 0.6529\n",
      "Epoch 21/26\n",
      "11/59 [====>.........................] - ETA: 3:49 - loss: 0.6758 - accuracy: 0.7109"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m acc_A_train, acc_A_val, history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_layer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36mmodel_train\u001b[1;34m(X_train, y_train, embedding_layer)\u001b[0m\n\u001b[0;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 28\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m26\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaskA.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1964\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc_A_train, acc_A_val, history = model_train(X_train, Y_train, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20104bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_Train(X_train, y_train):\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    class PositionalEncoding(keras.layers.Layer):\n",
    "        def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "            super().__init__(dtype=dtype, **kwargs)\n",
    "            if max_dims % 2 == 1: max_dims += 1 # max_dims must be even\n",
    "            p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "            pos_emb = np.empty((1, max_steps, max_dims))\n",
    "            pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
    "            pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
    "            self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "        def call(self, inputs):\n",
    "            shape = tf.shape(inputs)\n",
    "            return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]\n",
    "\n",
    "\n",
    "    model.add(SpatialDropout1D(0.2))   \n",
    "    \n",
    "    embed_size = 100; max_steps = 500; vocab_size = len(vocab)+1\n",
    "\n",
    "    encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "    decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "\n",
    "    embeddings = keras.layers.Embedding(vocab_size, embed_size,weights = [embedding_matrix])\n",
    "\n",
    "    encoder_embeddings = embeddings(encoder_inputs)\n",
    "    decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "    positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\n",
    "\n",
    "    encoder_in = positional_encoding(encoder_embeddings)\n",
    "    decoder_in = positional_encoding(decoder_embeddings)\n",
    "    \n",
    "    Z = encoder_in\n",
    "    for N in range(6):\n",
    "        Z = keras.layers.Attention(use_scale=True)([Z, Z])\n",
    "\n",
    "    encoder_outputs = Z\n",
    "    Z = decoder_in\n",
    "    for N in range(6):\n",
    "        query_seq_encoding = keras.layers.Attention(use_scale=True, causal=True)([Z, Z])\n",
    "        query_value_attention_seq = keras.layers.Attention(use_scale=True)([query_seq_encoding, encoder_outputs])\n",
    "\n",
    "#     outputs = keras.layers.TimeDistributed(\n",
    "#         keras.layers.Dense(vocab_size, activation=\"softmax\"))(Z)\n",
    "\n",
    "\n",
    "    # Reduce over the sequence axis to produce encodings of shape\n",
    "    # [batch_size, filters].\n",
    "    query_encoding = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "        query_seq_encoding)\n",
    "    query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "        query_value_attention_seq)\n",
    "\n",
    "    # Concatenate query and document encodings to produce a DNN input layer.\n",
    "    input_layer = tf.keras.layers.Concatenate()(\n",
    "        [query_encoding, query_value_attention])\n",
    "    \n",
    "\n",
    "    #         LSTM(128, dropout = 0.2, recurrent_dropout = 0.5)\n",
    "\n",
    "    #         LSTM(128,activation='tanh', recurrent_activation='sigmoid',\n",
    "    #              use_bias=True,dropout=0.5,recurrent_dropout=0.0)\n",
    "\n",
    "    model.add(Bidirectional(LSTM(128,dropout = 0.2, recurrent_dropout = 0.5)))\n",
    "     \n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    history = model.fit(X_train, y_train, validation_split = 0.2, epochs = 26, batch_size = 64)\n",
    "    model.summary()\n",
    "#     model.save('taskA.h5')\n",
    "    train_acc = history.history['accuracy'][-1]\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    return train_acc, val_acc, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3c9739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_A_train, acc_A_val, history = model_Train(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a3020",
   "metadata": {},
   "outputs": [],
   "source": [
    "    class PositionalEncoding(keras.layers.Layer):\n",
    "        def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "            super().__init__(dtype=dtype, **kwargs)\n",
    "            if max_dims % 2 == 1: max_dims += 1 # max_dims must be even\n",
    "            p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "            pos_emb = np.empty((1, max_steps, max_dims))\n",
    "            pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
    "            pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
    "            self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "        def call(self, inputs):\n",
    "            shape = tf.shape(inputs)\n",
    "            return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]\n",
    "    \n",
    "    embed_size = 100; max_steps = 500; vocab_size = len(vocab)+1\n",
    "\n",
    "    encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "    decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "\n",
    "    embeddings = keras.layers.Embedding(vocab_size, embed_size,weights = [embedding_matrix])\n",
    "\n",
    "    encoder_embeddings = embeddings(encoder_inputs)\n",
    "    decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "    positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\n",
    "\n",
    "    encoder_in = positional_encoding(encoder_embeddings)\n",
    "    decoder_in = positional_encoding(decoder_embeddings)\n",
    "    \n",
    "    Z = encoder_in\n",
    "    for N in range(6):\n",
    "        Z = keras.layers.Attention(use_scale=True)([Z, Z])\n",
    "\n",
    "    encoder_outputs = Z\n",
    "    Z = decoder_in\n",
    "    for N in range(6):\n",
    "        query_seq_encoding = keras.layers.Attention(use_scale=True, causal=True)([Z, Z])\n",
    "        query_value_attention_seq = keras.layers.Attention(use_scale=True)([query_seq_encoding, encoder_outputs])\n",
    "\n",
    "#     outputs = keras.layers.TimeDistributed(\n",
    "#         keras.layers.Dense(vocab_size, activation=\"softmax\"))(Z)\n",
    "\n",
    "\n",
    "    # Reduce over the sequence axis to produce encodings of shape\n",
    "    # [batch_size, filters].\n",
    "    query_encoding = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "        query_seq_encoding)\n",
    "    query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "        query_value_attention_seq)\n",
    "\n",
    "    # Concatenate query and document encodings to produce a DNN input layer.\n",
    "    input_layer = tf.keras.layers.Concatenate()(\n",
    "        [query_encoding, query_value_attention])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef5c4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
