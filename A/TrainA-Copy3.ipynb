{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d60291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load main.py\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import nltk\n",
    "import ekphrasis\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1415c",
   "metadata": {},
   "source": [
    "### Step1: Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f8f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "Path =os.path.dirname(os.getcwd())\n",
    "data_pathA=os.path.join(Path,'Datasets/A/twitter-2016train-A.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a8db65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>628949369883000832</td>\n",
       "      <td>negative</td>\n",
       "      <td>dear @Microsoft the newOoffice for Mac is grea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>628976607420645377</td>\n",
       "      <td>negative</td>\n",
       "      <td>@Microsoft how about you make a system that do...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>629023169169518592</td>\n",
       "      <td>negative</td>\n",
       "      <td>I may be ignorant on this issue but... should ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>629179223232479232</td>\n",
       "      <td>negative</td>\n",
       "      <td>Thanks to @microsoft, I just may be switching ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>629186282179153920</td>\n",
       "      <td>neutral</td>\n",
       "      <td>If I make a game as a #windows10 Universal App...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5863</th>\n",
       "      <td>639855845958885376</td>\n",
       "      <td>positive</td>\n",
       "      <td>@Racalto_SK ok good to know. Punting at MetLif...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5864</th>\n",
       "      <td>639979760735662080</td>\n",
       "      <td>neutral</td>\n",
       "      <td>everyone who sat around me at metlife was so a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5865</th>\n",
       "      <td>640196838260363269</td>\n",
       "      <td>neutral</td>\n",
       "      <td>what giants or niners fans would wanna go to t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5866</th>\n",
       "      <td>640975710354567168</td>\n",
       "      <td>positive</td>\n",
       "      <td>Anybody want a ticket for tomorrow Colombia vs...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5867</th>\n",
       "      <td>641034340068143104</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Mendez told me he'd drive me to MetLife on Sun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5868 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ID Sentiment  \\\n",
       "0     628949369883000832  negative   \n",
       "1     628976607420645377  negative   \n",
       "2     629023169169518592  negative   \n",
       "3     629179223232479232  negative   \n",
       "4     629186282179153920   neutral   \n",
       "...                  ...       ...   \n",
       "5863  639855845958885376  positive   \n",
       "5864  639979760735662080   neutral   \n",
       "5865  640196838260363269   neutral   \n",
       "5866  640975710354567168  positive   \n",
       "5867  641034340068143104   neutral   \n",
       "\n",
       "                                                   Text  label  \n",
       "0     dear @Microsoft the newOoffice for Mac is grea...      0  \n",
       "1     @Microsoft how about you make a system that do...      0  \n",
       "2     I may be ignorant on this issue but... should ...      0  \n",
       "3     Thanks to @microsoft, I just may be switching ...      0  \n",
       "4     If I make a game as a #windows10 Universal App...      1  \n",
       "...                                                 ...    ...  \n",
       "5863  @Racalto_SK ok good to know. Punting at MetLif...      2  \n",
       "5864  everyone who sat around me at metlife was so a...      1  \n",
       "5865  what giants or niners fans would wanna go to t...      1  \n",
       "5866  Anybody want a ticket for tomorrow Colombia vs...      2  \n",
       "5867  Mendez told me he'd drive me to MetLife on Sun...      1  \n",
       "\n",
       "[5868 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform data into df form\n",
    "dataA = pd.read_table(data_pathA,sep='\\t',header=None)\n",
    "dataA.columns = ['ID','Sentiment','Text']\n",
    "def add_label(sentiment):\n",
    "    if sentiment == 'negative':\n",
    "        return 0\n",
    "    elif sentiment == 'neutral':\n",
    "        return 1\n",
    "    elif sentiment == 'positive':\n",
    "        return 2\n",
    "\n",
    "dataA['label'] = dataA.Sentiment.apply(add_label)\n",
    "dataA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d65b7d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    3017\n",
       "1    2001\n",
       "0     850\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment distribution of data\n",
    "dataA.loc[:,'label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443edeb7",
   "metadata": {},
   "source": [
    "1Case conversion\n",
    "包含“India”和“india”的语料库如果不应用小写化，机器会把它们识别为两个独立的术语，而实际上它们都是同一个单词的不同形式，并且对应于同一个国家。小写化后，仅存在一种“India”实例，即“india”，简化了在语料库中找到所有提到印度时的任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24103376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCY\\AppData\\Roaming\\Python\\Python38\\site-packages\\ekphrasis\\classes\\tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCY\\AppData\\Roaming\\Python\\Python38\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "#import ekphrasis library\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcbb4e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HCY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "    stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b296cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(Texts):\n",
    "    token=[]\n",
    "    for Text in Texts:\n",
    "        words = [sentence for sentence in text_processor.pre_process_doc(Text) if (sentence!='s' and sentence!='\\'')]\n",
    "        words = [word for word in words if (word not in stop)]\n",
    "        token.append(words)\n",
    "    words=[word for words in token for word in words]\n",
    "    \n",
    "    print(\"All words: {}\".format(len(words)))\n",
    "    # Create Counter\n",
    "    counts = Counter(words)\n",
    "    print(\"Unique words: {}\".format(len(counts)))\n",
    "\n",
    "    Most_common= counts.most_common()[:30]\n",
    "    print(\"Top 30 most common words: {}\".format(Most_common))\n",
    "    \n",
    "    vocab = {word: num for num, word in enumerate(counts, 1)}\n",
    "    id2vocab = {v: k for k, v in vocab.items()}\n",
    "    return token,vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a39875da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All words: 96351\n",
      "Unique words: 11133\n",
      "Top 30 most common words: [('.', 5384), (',', 2844), ('<user>', 2245), ('<url>', 2206), ('<number>', 1556), ('<hashtag>', 1482), ('</hashtag>', 1482), ('<allcaps>', 1395), ('</allcaps>', 1395), ('<repeated>', 1381), ('!', 1313), ('-', 1094), ('may', 1087), ('tomorrow', 897), (':', 888), ('?', 859), ('\"', 634), ('th', 600), ('day', 572), ('1', 565), ('<date>', 526), ('going', 406), ('st', 383), ('apple', 381), ('&', 370), ('2', 369), ('see', 359), ('like', 347), ('friday', 344), ('amazon', 342)]\n"
     ]
    }
   ],
   "source": [
    "token,vocab=Tokenize(dataA.Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ca1c7",
   "metadata": {},
   "source": [
    "### Step2: Word2Vec Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e51b0eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk import word_tokenize\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcf25f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model=Word2Vec(token,window=5, min_count=1,workers = multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b4c56bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7432904, 9635100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.train(token, total_examples = len(token), epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b864413b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is summary of Word2Vec: Word2Vec(vocab=11133, vector_size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print('This is summary of Word2Vec: {}'.format(word2vec_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f349892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=word2vec_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ada745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.save_word2vec_format('Word2Vec.vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e06de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_path=os.path.join(Path,'Datasets/datastories.twitter.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f820ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_matrix = np.zeros((len(index), 100))\n",
    "embed_dict={}\n",
    "for word, i in index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embed_matrix[i] = word2vec_model.wv[word]\n",
    "        embed_dict[word] = word2vec_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0514fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d10aa858",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = pd.read_table(word_path,sep=' ',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c371b3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "word.set_index(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f5bb152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Embed_dict={}\n",
    "for i in range(word.shape[0]):\n",
    "    Embed_dict[word.index[i]]=word.iloc[i,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ded29",
   "metadata": {},
   "source": [
    "### Step3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4e8d7c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import LSTM\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# from transformers import AdamW as AdamW_HF#, get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46860607",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn. Module): \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_class, weight,dim_feedforward=512, \n",
    "                 num_head=2, num_layers=2, dropout=0.1, max_len=128, activation: str = \"relu\"):\n",
    "        super (Transformer, self).__init__()\n",
    "        self. embedding_dim = embedding_dim\n",
    "        self. embeddings = nn. Embedding (vocab_size, embedding_dim) # 词向量层\n",
    "        embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self. position_embedding = PositionalEncoding (embedding_dim, dropout,max_len) #位置编码层\n",
    "\n",
    "        #编码层:使用TransformerEncoder\n",
    "        encoder_layer = nn. TransformerEncoderLayer (hidden_dim, num_head, dim_feedforward, dropout, activation)\n",
    "        self.transformer = nn. TransformerEncoder (encoder_layer, num_layers)\n",
    "\n",
    "        #输出层\n",
    "        self.output = nn.Linear (hidden_dim, num_class)\n",
    "        \n",
    "    def forward (self, inputs, lengths): \n",
    "        inputs = torch.transpose (inputs, 0, 1)\n",
    "        # 与LSTM处理情况相同,输入数据的第1维是批次,需要转换为TransformerEncoder\n",
    "        #所需要的第1维是长度,第2维是批次的形状\n",
    "        hidden_states = self.embeddings(inputs) \n",
    "        hidden_states = self.position_embedding (hidden_states) \n",
    "        attention_mask = length_to_mask (lengths) == False\n",
    "        #根据批次中每个序列长度生成Mask矩阵\n",
    "        hidden_states = self.transformer (hidden_states, src_key_padding_mask= attention_mask)\n",
    "        hidden_states = hidden_states [0, :, :]\n",
    "        #取第一个标记的输出结果作为分类层的输入\n",
    "        output = self.output (hidden_states)\n",
    "        log_probs = F.log_softmax (output, dim=1) \n",
    "        return log_probs   \n",
    "    def length_to_mask(lengths):\n",
    "        max_len = torch.max (lengths)\n",
    "        mask = torch.arange (max_len) .expand (lengths. shape [0], max_len) < lengths.unsqueeze (1)\n",
    "        return mask\n",
    "    \n",
    "class PositionalEncoding (nn.Module): \n",
    "    def __init__(self, d_model, dropout=0.1, max_len=512): \n",
    "        super (PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros (max_len, d_model)\n",
    "        position = torch.arange (O, max_len, dtype=torch.float).unsqueeze (1) \n",
    "        div_term = torch.exp (torch.arange (0, d_model, 2).float () * (-math.log (10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin (position * div_term) # 对偶数位置编码\n",
    "        pe[:, 1::2] = torch.cos (position * div_term) #对奇数位置编码\n",
    "        pe = pe.unsqueeze (O). transpose (0, 1)\n",
    "        self.register_buffer('pe', pe) # 不对位置编码层求梯度\n",
    "        \n",
    "    def forward (self, x):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        x= x + self.pe[:x.size (0), :] # 输入的词向量与位置编码相加\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5188d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(vocab)+1\n",
    "embedding_dim=100\n",
    "hidden_dim=256\n",
    "num_class=2\n",
    "weight=\n",
    "Transformer(vocab_size, embedding_dim, hidden_dim, num_class,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a45ac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, weight):\n",
    "\n",
    "        super(BiLSTM_Attention, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 3)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Initialize the hidden state\n",
    "        self.w_omega = nn.Parameter(torch.Tensor(hidden_dim * 2, hidden_dim * 2))\n",
    "        self.u_omega = nn.Parameter(torch.Tensor(hidden_dim * 2, 1))\n",
    "\n",
    "        nn.init.uniform_(self.w_omega, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.u_omega, -0.1, 0.1)\n",
    "\n",
    "\n",
    "    def attention_net(self, x):       #x:[batch, seq_len, hidden_dim*2]\n",
    "\n",
    "        u = torch.tanh(torch.matmul(x, self.w_omega))         #[batch, seq_len, hidden_dim*2]\n",
    "        att = torch.matmul(u, self.u_omega)                   #[batch, seq_len, 1]\n",
    "        att_score = F.softmax(att, dim=1)\n",
    "\n",
    "        scored_x = x * att_score                              #[batch, seq_len, hidden_dim*2]\n",
    "\n",
    "        context = torch.sum(scored_x, dim=1)                  #[batch, hidden_dim*2]\n",
    "        return context\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        embedding = self.dropout(self.embedding(x))       #[seq_len, batch, embedding_dim]\n",
    "        #embedding = embedding + (0.2**0.5)*torch.randn(embedding.shape,device=device)\n",
    "\n",
    "        # output: [seq_len, batch, hidden_dim*2]     hidden/cell: [n_layers*2, batch, hidden_dim]\n",
    "        output, (final_hidden_state, final_cell_state) = self.rnn(embedding)\n",
    "        output = output.permute(1, 0, 2)                  #[batch, seq_len, hidden_dim*2]\n",
    "        \n",
    "        output = output + (0.2**0.5)*torch.randn(output.shape,device=device)\n",
    "\n",
    "        attn_output = self.attention_net(output)\n",
    "        logit = self.fc(attn_output)\n",
    "        return logit\n",
    "    \n",
    "    \n",
    "# Define LSTM Tokenizer\n",
    "def tokenizer_lstm(X, vocab, seq_len):\n",
    "    '''\n",
    "    Returns tokenized tensor with left/right padding at the specified sequence length\n",
    "    '''\n",
    "    X_tmp = np.zeros((len(X), seq_len), dtype=np.int64)\n",
    "    for i, text in enumerate(X):\n",
    "        tokens = [word for word in text_processor.pre_process_doc(text) if (word!='s' and word!='\\'')]\n",
    "        tokens = [word for word in tokens if (word not in stop)]\n",
    "        token_ids = [vocab[word] for word in tokens if word in Embed_dict.keys()]\n",
    "        end_idx = min(len(token_ids), seq_len)\n",
    "        start_idx = max(seq_len - len(token_ids), 0)\n",
    "        X_tmp[i,start_idx:] = token_ids[:end_idx]\n",
    "\n",
    "    return X_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cc1d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenizer_lstm(X, vocab, seq_len, padding):\n",
    "#     '''\n",
    "#     Returns tokenized tensor with left/right padding at the specified sequence length\n",
    "#     '''\n",
    "#     X_tmp = np.zeros((len(X), seq_len), dtype=np.int64)\n",
    "#     for i, text in enumerate(X):\n",
    "#         tokens = tokenize_text(text, 3) \n",
    "#         token_ids = [vocab[word] for word in tokens if word in word2idx.keys()]\n",
    "#         end_idx = min(len(token_ids), seq_len)\n",
    "#         if padding == 'right':\n",
    "#             X_tmp[i,:end_idx] = token_ids[:end_idx]\n",
    "#         elif padding == 'left':\n",
    "#             start_idx = max(seq_len - len(token_ids), 0)\n",
    "#             X_tmp[i,start_idx:] = token_ids[:end_idx]\n",
    "\n",
    "#     return torch.tensor(X_tmp, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d4ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tokenizer_lstm(dataA.Text, vocab, 100)###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a0b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.one_hot(dataA.label, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93237066",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y= np.array(Y)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size=0.1, random_state=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_layer(vocab, embeddings_index):\n",
    "    \"\"\"\n",
    "    Build embedding matrix and embedding layer\n",
    "    :param vocab_size: vocabulary size\n",
    "    :param tok: tokenizer\n",
    "    :param embeddings_index: embedding index\n",
    "    :return: embedding matrix and embedding layer\n",
    "    \"\"\"\n",
    "    #Build embedding matrix\n",
    "    vocab_size=len(vocab)+1\n",
    "    embedding_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, i in vocab.items():\n",
    "        try:\n",
    "            # Vector corresponds to word\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "        except:\n",
    "            embedding_vector = embeddings_index['<unk>']#['unknown']#['<unk>']\n",
    "        if embedding_vector is not None:\n",
    "            # Ensure vector of embedding_matrix row matches word index\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    # Build embedding layer\n",
    "    embedding_layer = Embedding(input_dim = vocab_size, output_dim = 100, weights = [embedding_matrix], input_length = 100, trainable=False)\n",
    "    return embedding_layer,embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8158cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer,embedding_matrix=build_embedding_layer(vocab, Embed_dict) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34afba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(X_train, y_train, embedding_layer):\n",
    "        \"\"\"\n",
    "        Train, validate and test BiLSTM model, calculate accuracy of training and validation set\n",
    "        :param X_train: tweet train data\n",
    "        :param y_train: sentiment label train data\n",
    "        :param embedding_layer: embedding layer\n",
    "        :param X_test: tweet test data\n",
    "        :param y_test: sentiment label test data\n",
    "        :return: accuracy, recall, precision, F1 score and history\n",
    "        \"\"\"\n",
    "        tf.debugging.set_log_device_placement(True)\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        model.add(SpatialDropout1D(0.2))\n",
    "        \n",
    "#         LSTM(128, dropout = 0.2, recurrent_dropout = 0.5)\n",
    "\n",
    "#         LSTM(128,activation='tanh', recurrent_activation='sigmoid',\n",
    "#              use_bias=True,dropout=0.5,recurrent_dropout=0.0)\n",
    "    \n",
    "        model.add(Bidirectional(LSTM(128,dropout = 0.2, recurrent_dropout = 0.5,return_sequences=True)))\n",
    "        model.add(Bidirectional(LSTM(128,dropout = 0.2,recurrent_dropout = 0.5)))\n",
    "        \n",
    "        model.add(Dense(3, activation = 'softmax'))\n",
    "        model.summary()\n",
    "        model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "        history = model.fit(X_train, y_train, validation_split = 0.2, epochs = 26, batch_size = 256)\n",
    "        model.save('taskA.h5')\n",
    "        train_acc = history.history['accuracy'][-1]\n",
    "        val_acc = history.history['val_accuracy'][-1]\n",
    "        return train_acc, val_acc, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c45cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_A_train, acc_A_val, history = model_train(X_train, Y_train, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20104bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_Train(X_train, y_train):\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    class PositionalEncoding(keras.layers.Layer):\n",
    "        def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "            super().__init__(dtype=dtype, **kwargs)\n",
    "            if max_dims % 2 == 1: max_dims += 1 # max_dims must be even\n",
    "            p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "            pos_emb = np.empty((1, max_steps, max_dims))\n",
    "            pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
    "            pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
    "            self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "        def call(self, inputs):\n",
    "            shape = tf.shape(inputs)\n",
    "            return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]\n",
    "\n",
    "\n",
    "    model.add(SpatialDropout1D(0.2))   \n",
    "    \n",
    "    embed_size = 100; max_steps = 500; vocab_size = len(vocab)+1\n",
    "\n",
    "    encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "    decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "\n",
    "    embeddings = keras.layers.Embedding(vocab_size, embed_size,weights = [embedding_matrix])\n",
    "\n",
    "    encoder_embeddings = embeddings(encoder_inputs)\n",
    "    decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "    positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\n",
    "\n",
    "    encoder_in = positional_encoding(encoder_embeddings)\n",
    "    decoder_in = positional_encoding(decoder_embeddings)\n",
    "    \n",
    "    Z = encoder_in\n",
    "    for N in range(6):\n",
    "        Z = keras.layers.Attention(use_scale=True)([Z, Z])\n",
    "\n",
    "    encoder_outputs = Z\n",
    "    Z = decoder_in\n",
    "    for N in range(6):\n",
    "        query_seq_encoding = keras.layers.Attention(use_scale=True, causal=True)([Z, Z])\n",
    "        query_value_attention_seq = keras.layers.Attention(use_scale=True)([query_seq_encoding, encoder_outputs])\n",
    "\n",
    "#     outputs = keras.layers.TimeDistributed(\n",
    "#         keras.layers.Dense(vocab_size, activation=\"softmax\"))(Z)\n",
    "\n",
    "\n",
    "    # Reduce over the sequence axis to produce encodings of shape\n",
    "    # [batch_size, filters].\n",
    "    query_encoding = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "        query_seq_encoding)\n",
    "    query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "        query_value_attention_seq)\n",
    "\n",
    "    # Concatenate query and document encodings to produce a DNN input layer.\n",
    "    input_layer = tf.keras.layers.Concatenate()(\n",
    "        [query_encoding, query_value_attention])\n",
    "    \n",
    "\n",
    "    #         LSTM(128, dropout = 0.2, recurrent_dropout = 0.5)\n",
    "\n",
    "    #         LSTM(128,activation='tanh', recurrent_activation='sigmoid',\n",
    "    #              use_bias=True,dropout=0.5,recurrent_dropout=0.0)\n",
    "\n",
    "    model.add(Bidirectional(LSTM(128,dropout = 0.2, recurrent_dropout = 0.5)))\n",
    "     \n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    history = model.fit(X_train, y_train, validation_split = 0.2, epochs = 26, batch_size = 64)\n",
    "    model.summary()\n",
    "#     model.save('taskA.h5')\n",
    "    train_acc = history.history['accuracy'][-1]\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    return train_acc, val_acc, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3c9739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_A_train, acc_A_val, history = model_Train(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef5c4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
