{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bc22ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.6.0\n",
      "Eager mode:  True\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import nltk\n",
    "import ekphrasis\n",
    "from collections import Counter\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk import word_tokenize\n",
    "import multiprocessing\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, SpatialDropout1D, Bidirectional\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a5f881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    \n",
    "# Step1: Data Pre-processing    \n",
    "    def folderdir(self):\n",
    "        \n",
    "        Path =os.path.dirname(os.getcwd())\n",
    "        \n",
    "        return Path\n",
    "    def read_data(self,Path,address):\n",
    "        '''\n",
    "        Read data from given address\n",
    "        '''\n",
    "        data_path=os.path.join(Path,'Datasets/A/twitter-2016train-A.txt')\n",
    "        data = pd.read_table(data_path,sep='\\t',header=None)\n",
    "        \n",
    "        return data\n",
    "        \n",
    "\n",
    "    def add_label(sentiment):\n",
    "        if sentiment == 'negative':\n",
    "            return 0\n",
    "        elif sentiment == 'neutral':\n",
    "            return 1\n",
    "        elif sentiment == 'positive':\n",
    "            return 2\n",
    "    \n",
    "    def label_distrib(self,dataA):\n",
    "        distrib=dataA.loc[:,['Sentiment','label']].value_counts().to_dict()\n",
    "        \n",
    "        return distrib\n",
    "    \n",
    "    def text_processor(self):\n",
    "        #import ekphrasis library\n",
    "        from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "        from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "        from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "        text_processor = TextPreProcessor(\n",
    "            # terms that will be normalized\n",
    "            normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "                'time', 'url', 'date', 'number'],\n",
    "            # terms that will be annotated\n",
    "            annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "                'emphasis', 'censored'},\n",
    "            fix_html=True,  # fix HTML tokens\n",
    "\n",
    "            # corpus from which the word statistics are going to be used \n",
    "            # for word segmentation \n",
    "            segmenter=\"twitter\", \n",
    "\n",
    "            # corpus from which the word statistics are going to be used \n",
    "            # for spell correction\n",
    "            corrector=\"twitter\", \n",
    "\n",
    "            unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "            unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "            spell_correct_elong=False,  # spell correction for elongated words\n",
    "\n",
    "            # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "            # the tokenizer, should take as input a string and return a list of tokens\n",
    "            tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "\n",
    "            # list of dictionaries, for replacing tokens extracted from the text,\n",
    "            # with other expressions. You can pass more than one dictionaries.\n",
    "            dicts=[emoticons]\n",
    "        )\n",
    "        return text_processor\n",
    "    \n",
    "    def Tokenize(self,Texts):\n",
    "        token=[]\n",
    "        for Text in Texts:\n",
    "            words = [sentence for sentence in self.text_processor.pre_process_doc(Text) if (sentence!='s' and sentence!='\\'')]\n",
    "            token.append(words)\n",
    "        words=[word for words in token for word in words]\n",
    "\n",
    "        print(\"All words: {}\".format(len(words)))\n",
    "        # Create Counter\n",
    "        counts = Counter(words)\n",
    "        print(\"Unique words: {}\".format(len(counts)))\n",
    "\n",
    "        Most_common= counts.most_common()[:30]\n",
    "        print(\"Top 30 most common words: {}\".format(Most_common))\n",
    "\n",
    "        vocab = {word: num for num, word in enumerate(counts, 1)}\n",
    "        id2vocab = {v: k for k, v in vocab.items()}\n",
    "        return token,vocab\n",
    "    \n",
    "# Step2: Word2Vec Pretraining    \n",
    "    def word2vec(self,token,window,min_count,epochs):\n",
    "        word2vec_model=Word2Vec(token,window=window, min_count=min_count,workers = multiprocessing.cpu_count())\n",
    "        word2vec_model.train(token, total_examples = len(token), epochs = epochs)\n",
    "        print('This is summary of Word2Vec: {}'.format(word2vec_model))\n",
    "        \n",
    "        index=word2vec_model.wv.key_to_index\n",
    "        word2vec_model.wv.save_word2vec_format('Word2Vec.vector')\n",
    "        embed_matrix = np.zeros((len(index), 100))\n",
    "        \n",
    "        embed_dict={}\n",
    "        for word, i in index.items():\n",
    "            if word in word2vec_model.wv:\n",
    "                embed_matrix[i] = word2vec_model.wv[word]\n",
    "                embed_dict[word] = word2vec_model.wv[word]\n",
    "                \n",
    "        del word2vec_model # del model to Save RAM\n",
    "        \n",
    "        return embed_dict\n",
    "    \n",
    "# Step3: split train and test sets\n",
    "    def tokenizer_lstm(self,X, vocab, seq_len,embed_dict):\n",
    "        '''\n",
    "        Returns tokenized tensor with left padding\n",
    "        '''\n",
    "        X_tmp = np.zeros((len(X), seq_len), dtype=np.int64)\n",
    "        for i, text in enumerate(X):\n",
    "            tokens = [word for word in self.text_processor.pre_process_doc(text) if (word!='s' and word!='\\'')]\n",
    "    #         tokens = [word for word in tokens if (word not in stop)]\n",
    "            token_ids = [vocab[word] for word in tokens if word in embed_dict.keys()]###\n",
    "            end_idx = min(len(token_ids), seq_len)\n",
    "            start_idx = max(seq_len - len(token_ids), 0)\n",
    "            X_tmp[i,start_idx:] = token_ids[:end_idx]\n",
    "\n",
    "        return X_tmp\n",
    "    \n",
    "    def split(Text,vocab,label,seq_len,embed_dict):\n",
    "        \n",
    "        X=self.tokenizer_lstm(Text, vocab, seq_len)\n",
    "        \n",
    "        Y = tf.one_hot(label, depth=3)\n",
    "        Y= np.array(Y)\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size=0.1, random_state=1000) \n",
    "        \n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "    \n",
    "    def Final_PreProcess(self,address,window,min_count,epochs,seq_len):    \n",
    "        PathA=self.folderdir()\n",
    "        dataA=self.read_data(PathA,address)\n",
    "        dataA.columns = ['ID','Sentiment','Text','Nan']\n",
    "        dataA['label'] = dataA.Sentiment.apply(self.add_label)\n",
    "        distrib=self.label_distrib(dataA)\n",
    "        token,vocab=self.Tokenize(dataA.Text)\n",
    "        \n",
    "        embed_dict=self.word2vec(token,window,min_count,epochs)\n",
    "        X_train, X_test, Y_train, Y_test=self.split(dataA.Text,vocab,dataA.label,seq_len,embed_dict)\n",
    "        return X_train, X_test, Y_train, Y_test,vocab,embed_dict\n",
    "        \n",
    "# address='Datasets/A/twitter-2016train-A.txt'\n",
    "# window=5\n",
    "# min_count=1\n",
    "# epochs=100\n",
    "# seq_len=100\n",
    "\n",
    "# Step4: Training\n",
    "    def Final_Train(self,vocab,embed_dict,X_train,y_train, embedding_layer,batch_size,epochs):\n",
    "        embedding_layer=self.build_embedding_layer(vocab,embed_dict)\n",
    "        train_acc, val_acc, train_rec, val_rec, train_pre, val_pre,history=self.model_train(X_train, \n",
    "                                                                                            y_train, embedding_layer,batch_size,epochs)\n",
    "        return train_acc, val_acc, train_rec, val_rec, train_pre, val_pre,history\n",
    "    \n",
    "    def build_embedding_layer(self, vocab, embed_dict):\n",
    "        \"\"\"\n",
    "        Build embedding matrix and embedding layer\n",
    "        :param vocab_size: vocabulary size\n",
    "        :param tok: tokenizer\n",
    "        :param embeddings_index: embedding index\n",
    "        :return: embedding matrix and embedding layer\n",
    "        \"\"\"\n",
    "        #Build embedding matrix\n",
    "        vocab_size=len(vocab)+1\n",
    "        embedding_matrix = np.zeros((vocab_size, 100))\n",
    "        for word, i in vocab.items():\n",
    "            # Vector corresponds to word\n",
    "            embedding_vector = embed_dict.get(word)###,embed_dict['<unk>']\n",
    "\n",
    "            if embedding_vector is not None:\n",
    "                # Ensure vector of embedding_matrix row matches word index\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        # Build embedding layer\n",
    "        embedding_layer = Embedding(input_dim = vocab_size, output_dim = 100, weights = [embedding_matrix], input_length = 100, trainable=False)\n",
    "\n",
    "        return embedding_layer\n",
    "\n",
    "    def model_train(self,X_train, y_train, embedding_layer,batch_size,epochs):\n",
    "        \"\"\"\n",
    "        Train, validate and test BiLSTM model, calculate accuracy of training and validation set\n",
    "        :param X_train: tweet train data\n",
    "        :param y_train: sentiment label train data\n",
    "        :param embedding_layer: embedding layer\n",
    "        :param X_test: tweet test data\n",
    "        :param y_test: sentiment label test data\n",
    "        :return: accuracy, recall, precision, F1 score and history\n",
    "        \"\"\"\n",
    "        tf.debugging.set_log_device_placement(True)\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        model.add(SpatialDropout1D(0.2))\n",
    "        \n",
    "        model.add(Bidirectional(LSTM(128,dropout = 0.5,return_sequences=True)))\n",
    "        model.add(Bidirectional(LSTM(64,dropout = 0.5)))  #27 loss: 0.6735 - accuracy: 0.7028 - val_loss: 0.7640 - val_accuracy: 0.6669\n",
    "\n",
    "\n",
    "    \n",
    "        model.add(Dense(3, activation = 'softmax'))\n",
    "        model.summary()\n",
    "        \n",
    "\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "                      metrics = ['Recall','Accuracy','Precision'])\n",
    "        \n",
    "        history = model.fit(X_train, y_train, validation_split = 0.2, epochs = epochs, batch_size = batch_size)\n",
    "        \n",
    "        model.save('taskA.h5'.format(25))\n",
    "        \n",
    "        train_acc = history.history['Accuracy'][-1]\n",
    "        val_acc = history.history['val_Accuracy'][-1]\n",
    "        \n",
    "        train_rec = history.history['recall'][-1]\n",
    "        val_rec = history.history['val_recall'][-1]\n",
    "        \n",
    "        train_pre = history.history['precision'][-1]\n",
    "        val_pre = history.history['val_precision'][-1]\n",
    "        \n",
    "        return train_acc, val_acc, train_rec, val_rec, train_pre, val_pre,history    \n",
    "    \n",
    "    def sketch_model():\n",
    "        return\n",
    "    \n",
    "# Step5: prediction and evaluation\n",
    "    def pred_eval():\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b131347f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "folderdir() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      6\u001b[0m seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m----> 7\u001b[0m X_train, X_test, Y_train, Y_test\u001b[38;5;241m=\u001b[39m\u001b[43mClassA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFinal_PreProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mA.Final_PreProcess\u001b[1;34m(self, address, window, min_count, epochs, seq_len)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mFinal_PreProcess\u001b[39m(\u001b[38;5;28mself\u001b[39m,address,window,min_count,epochs,seq_len):    \n\u001b[1;32m--> 135\u001b[0m     PathA\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfolderdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m     dataA\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_data(PathA,address)\n\u001b[0;32m    137\u001b[0m     dataA\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNan\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: folderdir() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "# ClassA=A\n",
    "# address='Datasets/A/twitter-2016train-A.txt'\n",
    "# window=5\n",
    "# min_count=1\n",
    "# epochs=100\n",
    "# seq_len=100\n",
    "# X_train, X_test, Y_train, Y_test=ClassA.Final_PreProcess(A,address,window,min_count,epochs,seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0291e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
