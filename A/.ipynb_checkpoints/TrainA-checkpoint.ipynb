{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d60291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load main.py\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import nltk\n",
    "import ekphrasis\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1415c",
   "metadata": {},
   "source": [
    "### Step1: Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f8f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "Path =os.path.dirname(os.getcwd())\n",
    "data_pathA=os.path.join(Path,'Datasets/A/twitter-2016train-A.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a8db65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transform data into df form\n",
    "dataA = pd.read_table(data_pathA,sep='\\t',header=0)\n",
    "dataA.columns = ['ID','Sentiment','Text']\n",
    "def add_label(sentiment):\n",
    "    if sentiment == 'negative':\n",
    "        return 0\n",
    "    elif sentiment == 'neutral':\n",
    "        return 1\n",
    "    elif sentiment == 'positive':\n",
    "        return 2\n",
    "\n",
    "dataA['label'] = dataA.Sentiment.apply(add_label)\n",
    "dataA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b7d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sentiment distribution of data\n",
    "dataA.loc[:,'label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443edeb7",
   "metadata": {},
   "source": [
    "1Case conversion\n",
    "包含“India”和“india”的语料库如果不应用小写化，机器会把它们识别为两个独立的术语，而实际上它们都是同一个单词的不同形式，并且对应于同一个国家。小写化后，仅存在一种“India”实例，即“india”，简化了在语料库中找到所有提到印度时的任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24103376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ekphrasis library\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b296cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(Texts):\n",
    "    token=[]\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "    stop = set(stopwords.words('english'))\n",
    "    \n",
    "    for Text in Texts:\n",
    "        words = [sentence for sentence in text_processor.pre_process_doc(Text) if (sentence!='s' and sentence!='\\'')]\n",
    "        words = [word for word in words if (word not in stop)]\n",
    "        token.append(words)\n",
    "    words=[word for words in token for word in words]\n",
    "    \n",
    "    print(\"All words: {}\".format(len(words)))\n",
    "    # Create Counter\n",
    "    counts = Counter(words)\n",
    "    print(\"Unique words: {}\".format(len(counts)))\n",
    "\n",
    "    Most_common= counts.most_common()[:30]\n",
    "    print(\"Top 30 most common words: {}\".format(Most_common))\n",
    "    \n",
    "    vocab = {word: num for num, word in enumerate(counts, 1)}\n",
    "    id2vocab = {v: k for k, v in vocab.items()}\n",
    "    return token,vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "token,vocab=Tokenize(dataA.Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ca1c7",
   "metadata": {},
   "source": [
    "### Step2: Word2Vec Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b0eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk import word_tokenize\n",
    "import multiprocessing\n",
    "import tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf25f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model=Word2Vec(token,window=5, min_count=1,workers = multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4c56bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.train(token, total_examples = len(token), epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b864413b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('This is summary of Word2Vec: {}'.format(word2vec_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f349892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=word2vec_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.save_word2vec_format('Word2Vec.vector')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ded29",
   "metadata": {},
   "source": [
    "### Step3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e8d7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, SpatialDropout1D, Bidirectional\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import transformers\n",
    "import tensorflow_hub as hub\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "display.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c467cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_matrix = np.zeros((len(index), 100))\n",
    "embed_dict={}\n",
    "for word, i in index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embed_matrix[i] = word2vec_model.wv[word]\n",
    "        embed_dict[word] = word2vec_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac1e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize texts\n",
    "def tokenize(tweet):\n",
    "    \"\"\"\n",
    "    Vectorize texts\n",
    "    :param df_tweet: The tweet text df['tweet']\n",
    "    :return: Tweet texts after vectorizing, vocabulary size\n",
    "    \"\"\"\n",
    "    tok = Tokenizer()\n",
    "    # Create vocabulary index based on word frequency\n",
    "    tok.fit_on_texts(tweet)\n",
    "    # Convert each text to a sequence of integers\n",
    "    X = pad_sequences(tok.texts_to_sequences(tweet), maxlen=100)\n",
    "    # Vocabulary size\n",
    "    vocab_size = len(tok.word_index) + 1\n",
    "    return X, vocab_size, tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, vocab_size, tok = tokenize(dataA.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "token,vocab=Tokenize(dataA.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3c852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_lstm(X, vocab, seq_len, padding):\n",
    "    '''\n",
    "    Returns tokenized tensor with left/right padding at the specified sequence length\n",
    "    '''\n",
    "    X_tmp = np.zeros((len(X), seq_len), dtype=np.int64)\n",
    "    for i, text in enumerate(X):\n",
    "        tokens = [word for word in text_processor.pre_process_doc(text) if (word!='s' and word!='\\'')]\n",
    "        token_ids = [vocab[word] for word in tokens if word in embed_dict.keys()]\n",
    "        end_idx = min(len(token_ids), seq_len)\n",
    "        if padding == 'right':\n",
    "            X_tmp[i,:end_idx] = token_ids[:end_idx]\n",
    "        elif padding == 'left':\n",
    "            start_idx = max(seq_len - len(token_ids), 0)\n",
    "            X_tmp[i,start_idx:] = token_ids[:end_idx]\n",
    "\n",
    "    return X_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tokenizer_lstm(dataA.Text, vocab, 100, 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c0933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(vocab)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a0b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.one_hot(dataA.label, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93237066",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y= np.array(Y)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size=0.1, random_state=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdf1b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_layer(vocab_size, tok, embeddings_index):\n",
    "    \"\"\"\n",
    "    Build embedding matrix and embedding layer\n",
    "    :param vocab_size: vocabulary size\n",
    "    :param tok: tokenizer\n",
    "    :param embeddings_index: embedding index\n",
    "    :return: embedding matrix and embedding layer\n",
    "    \"\"\"\n",
    "    #Build embedding matrix\n",
    "    embedding_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, i in tok.word_index.items():\n",
    "        try:\n",
    "            # Vector corresponds to word\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "        except:\n",
    "            embedding_vector = embeddings_index['unknown']\n",
    "        if embedding_vector is not None:\n",
    "            # Ensure vector of embedding_matrix row matches word index\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    # Build embedding layer\n",
    "    embedding_layer = Embedding(input_dim = vocab_size, output_dim = 100, weights = [embedding_matrix], input_length = 100, trainable=False)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ffb3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_layer(vocab_size, tok, embeddings_index):\n",
    "    \"\"\"\n",
    "    Build embedding matrix and embedding layer\n",
    "    :param vocab_size: vocabulary size\n",
    "    :param tok: tokenizer\n",
    "    :param embeddings_index: embedding index\n",
    "    :return: embedding matrix and embedding layer\n",
    "    \"\"\"\n",
    "    #Build embedding matrix\n",
    "    embedding_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, i in vocab.items(): #tok.word_index.items():\n",
    "        try:\n",
    "            # Vector corresponds to word\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "        except:\n",
    "            embedding_vector = embeddings_index['unknown']\n",
    "        if embedding_vector is not None:\n",
    "            # Ensure vector of embedding_matrix row matches word index\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    # Build embedding layer\n",
    "    embedding_layer = Embedding(input_dim = vocab_size, output_dim = 100, weights = [embedding_matrix], input_length = 100, trainable=False)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4065fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tok.word_index.items():\n",
    "    try:\n",
    "        # Vector corresponds to word\n",
    "        embedding_vector = embed_dict.get(word)\n",
    "    except:\n",
    "        embedding_vector = embed_dict['unknown']\n",
    "    if embedding_vector is not None:\n",
    "        # Ensure vector of embedding_matrix row matches word index\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "# Build embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcd2578",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer=build_embedding_layer(vocab_size, tok, embed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f64950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(X_train, y_train, embedding_layer):\n",
    "        \"\"\"\n",
    "        Train, validate and test BiLSTM model, calculate accuracy of training and validation set\n",
    "        :param X_train: tweet train data\n",
    "        :param y_train: sentiment label train data\n",
    "        :param embedding_layer: embedding layer\n",
    "        :param X_test: tweet test data\n",
    "        :param y_test: sentiment label test data\n",
    "        :return: accuracy, recall, precision, F1 score and history\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        model.add(SpatialDropout1D(0.2))\n",
    "        \n",
    "#         model.add(Bidirectional(LSTM(128, dropout = 0.2, recurrent_dropout = 0.5)))\n",
    "        forward_layer = LSTM(10, return_sequences=True,dropout = 0.2, recurrent_dropout = 0.5)\n",
    "        backward_layer = LSTM(10, activation='relu', return_sequences=True,dropout = 0.2, recurrent_dropout = 0.5,\n",
    "                           go_backwards=True)\n",
    "        model.add(Bidirectional(forward_layer, backward_layer=backward_layer))\n",
    "        \n",
    "        model.add(Dense(3, activation = 'softmax'))\n",
    "        model.summary()\n",
    "        model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "        \n",
    "        history = model.fit(X_train, y_train, validation_split = 0.2, epochs = 26, batch_size = 256)\n",
    "        model.save('taskA.h5')\n",
    "        train_acc = history.history['accuracy'][-1]\n",
    "        val_acc = history.history['val_accuracy'][-1]\n",
    "        return train_acc, val_acc, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa212071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(X_train, y_train, embedding_layer):\n",
    "        \"\"\"\n",
    "        Train, validate and test BiLSTM model, calculate accuracy of training and validation set\n",
    "        :param X_train: tweet train data\n",
    "        :param y_train: sentiment label train data\n",
    "        :param embedding_layer: embedding layer\n",
    "        :param X_test: tweet test data\n",
    "        :param y_test: sentiment label test data\n",
    "        :return: accuracy, recall, precision, F1 score and history\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        model.add(SpatialDropout1D(0.2))\n",
    "        \n",
    "        model.add(Bidirectional(LSTM(128, dropout = 0.5, recurrent_dropout = 0.0,activation='tanh',recurrent_activation='sigmoid')))\n",
    "#         forward_layer = LSTM(10, return_sequences=True)\n",
    "#         backward_layer = LSTM(10, activation='relu', return_sequences=True,\n",
    "#                            go_backwards=True)\n",
    "#         model.add(Bidirectional(forward_layer, backward_layer=backward_layer,input_shape=(5, 10)))\n",
    "        \n",
    "        model.add(Dense(3, activation = 'softmax'))\n",
    "        model.summary()\n",
    "        model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "        \n",
    "        history = model.fit(X_train, y_train, validation_split = 0.2, epochs = 26, batch_size = 256)\n",
    "        model.save('taskA.h5')\n",
    "        train_acc = history.history['accuracy'][-1]\n",
    "        val_acc = history.history['val_accuracy'][-1]\n",
    "        return train_acc, val_acc, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6bad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))\n",
    "# model.add(Bidirectional(LSTM(10)))\n",
    "# model.add(Dense(5))\n",
    "# model.add(Activation('softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# # With custom backward layer\n",
    "# model = Sequential()\n",
    "# forward_layer = LSTM(10, return_sequences=True)\n",
    "# backward_layer = LSTM(10, activation='relu', return_sequences=True,\n",
    "#                    go_backwards=True)\n",
    "# model.add(Bidirectional(forward_layer, backward_layer=backward_layer,\n",
    "#                      input_shape=(5, 10)))\n",
    "# model.add(Dense(5))\n",
    "# model.add(Activation('softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666ce7f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acc_A_train, acc_A_val, history = model_train(X_train, Y_train, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32c4fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c137c2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW as AdamW_HF, get_linear_schedule_with_warmup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
