{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dependency of the preprocessing for BERT inputs\n",
    "!pip install -q -U \"tensorflow-text==2.8.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94441a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tf-models-official==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2d60291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load main.py\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import nltk\n",
    "import ekphrasis\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1415c",
   "metadata": {},
   "source": [
    "### Step1: Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f8f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "Path =os.path.dirname(os.getcwd())\n",
    "data_pathA=os.path.join(Path,'Datasets/A/twitter-2016train-A.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "18a8db65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>628976607420645377</td>\n",
       "      <td>negative</td>\n",
       "      <td>@Microsoft how about you make a system that do...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>629023169169518592</td>\n",
       "      <td>negative</td>\n",
       "      <td>I may be ignorant on this issue but... should ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>629179223232479232</td>\n",
       "      <td>negative</td>\n",
       "      <td>Thanks to @microsoft, I just may be switching ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>629186282179153920</td>\n",
       "      <td>neutral</td>\n",
       "      <td>If I make a game as a #windows10 Universal App...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>629226490152914944</td>\n",
       "      <td>positive</td>\n",
       "      <td>Microsoft, I may not prefer your gaming branch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5862</th>\n",
       "      <td>639855845958885376</td>\n",
       "      <td>positive</td>\n",
       "      <td>@Racalto_SK ok good to know. Punting at MetLif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5863</th>\n",
       "      <td>639979760735662080</td>\n",
       "      <td>neutral</td>\n",
       "      <td>everyone who sat around me at metlife was so a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5864</th>\n",
       "      <td>640196838260363269</td>\n",
       "      <td>neutral</td>\n",
       "      <td>what giants or niners fans would wanna go to t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5865</th>\n",
       "      <td>640975710354567168</td>\n",
       "      <td>positive</td>\n",
       "      <td>Anybody want a ticket for tomorrow Colombia vs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5866</th>\n",
       "      <td>641034340068143104</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Mendez told me he'd drive me to MetLife on Sun...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5867 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ID Sentiment  \\\n",
       "0     628976607420645377  negative   \n",
       "1     629023169169518592  negative   \n",
       "2     629179223232479232  negative   \n",
       "3     629186282179153920   neutral   \n",
       "4     629226490152914944  positive   \n",
       "...                  ...       ...   \n",
       "5862  639855845958885376  positive   \n",
       "5863  639979760735662080   neutral   \n",
       "5864  640196838260363269   neutral   \n",
       "5865  640975710354567168  positive   \n",
       "5866  641034340068143104   neutral   \n",
       "\n",
       "                                                   Text  label  \n",
       "0     @Microsoft how about you make a system that do...     -1  \n",
       "1     I may be ignorant on this issue but... should ...     -1  \n",
       "2     Thanks to @microsoft, I just may be switching ...     -1  \n",
       "3     If I make a game as a #windows10 Universal App...      0  \n",
       "4     Microsoft, I may not prefer your gaming branch...      1  \n",
       "...                                                 ...    ...  \n",
       "5862  @Racalto_SK ok good to know. Punting at MetLif...      1  \n",
       "5863  everyone who sat around me at metlife was so a...      0  \n",
       "5864  what giants or niners fans would wanna go to t...      0  \n",
       "5865  Anybody want a ticket for tomorrow Colombia vs...      1  \n",
       "5866  Mendez told me he'd drive me to MetLife on Sun...      0  \n",
       "\n",
       "[5867 rows x 4 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform data into df form\n",
    "dataA = pd.read_table(data_pathA,sep='\\t',header=0)\n",
    "dataA.columns = ['ID','Sentiment','Text']\n",
    "def add_label(sentiment):\n",
    "    if sentiment == 'negative':\n",
    "        return -1\n",
    "    elif sentiment == 'neutral':\n",
    "        return 0\n",
    "    elif sentiment == 'positive':\n",
    "        return 1\n",
    "\n",
    "dataA['label'] = dataA.Sentiment.apply(add_label)\n",
    "dataA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d65b7d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    3017\n",
       " 0    2001\n",
       "-1     849\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment distribution of data\n",
    "dataA.loc[:,'label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443edeb7",
   "metadata": {},
   "source": [
    "1Case conversion\n",
    "包含“India”和“india”的语料库如果不应用小写化，机器会把它们识别为两个独立的术语，而实际上它们都是同一个单词的不同形式，并且对应于同一个国家。小写化后，仅存在一种“India”实例，即“india”，简化了在语料库中找到所有提到印度时的任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24103376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "#import ekphrasis library\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150aedc3",
   "metadata": {},
   "source": [
    "# 重点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d15deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text=[sentence for Text in dataA.Text for sentence in text_processor.pre_process_doc(Text) if (word!='s' and word!='\\'')]\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "words = [word for word in Text if (word not in stop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "85d75936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizaing: 100%|███████████████████████████████████████████████████████████████| 5867/5867 [00:01<00:00, 3750.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of all words: 140263\n",
      "The number of unique words: 11186\n",
      "Top 40 frequent words: ['.', 'the', ',', 'to', 'i', '<user>', '<url>', 'a', 'on', 'and', 'in', '<number>', '<hashtag>', '</hashtag>', 'of', '<repeated>', 'is', '!', 'for', 'it', 'you', 'may', '-', 'not', 'with', 'be', ':', 'tomorrow', 'at', '?', 'have', 'my', 'will', 'that', '\"', 'but', 'th', 'day', 'this', '1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "#Define the tokenzation function\n",
    "def tokenize_text(text, option):\n",
    "    '''\n",
    "    Tokenize the input text as per specified option\n",
    "        1: Use python split() function\n",
    "        2: Use regex to extract alphabets plus 's and 't\n",
    "        3: Use ekphrasis text_processor.pre_process_doc\n",
    "        4: Use NLTK word_tokenize(), remove stop words and apply lemmatization\n",
    "    '''\n",
    "    if option == 1:\n",
    "        return text.split()\n",
    "    elif option == 2:\n",
    "        return re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', text)\n",
    "    elif option == 3:\n",
    "        return [word for word in text_processor.pre_process_doc(text) if (word!='s' and word!='\\'')]\n",
    "    elif option == 4:\n",
    "        words = [word for word in word_tokenize(text) if (word.isalpha()==1)]\n",
    "        # Remove stop words\n",
    "        stop = set(stopwords.words('english'))\n",
    "        words = [word for word in words if (word not in stop)]\n",
    "        # Lemmatize words (first noun, then verb)\n",
    "        wnl = nltk.stem.WordNetLemmatizer()\n",
    "        lemmatized = [wnl.lemmatize(wnl.lemmatize(word, 'n'), 'v') for word in words]\n",
    "        return lemmatized\n",
    "    else:\n",
    "        print(\"Please specify option value between 1 and 4\")\n",
    "        return []\n",
    "# Create vocabulary to int dictionary\n",
    "def create_vocab(messages, show_graph=False):\n",
    "    corpus = []\n",
    "    for message in tqdm(messages, desc=\"Tokenizaing\"):\n",
    "        tokens = tokenize_text(message, 3) # Use option 3\n",
    "        corpus.extend(tokens)\n",
    "    print(\"The number of all words: {}\".format(len(corpus)))\n",
    "\n",
    "    # Create Counter\n",
    "    counts = Counter(corpus)\n",
    "    print(\"The number of unique words: {}\".format(len(counts)))\n",
    "\n",
    "    # Create BoW\n",
    "    bow = sorted(counts, key=counts.get, reverse=True)\n",
    "    print(\"Top 40 frequent words: {}\".format(bow[:40]))\n",
    "\n",
    "    # Indexing vocabrary, starting from 1.\n",
    "    vocab = {word: ii for ii, word in enumerate(counts, 1)}\n",
    "    id2vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    if show_graph:\n",
    "        from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "        import seaborn as sns\n",
    "        # Generate Word Cloud image\n",
    "        text = \" \".join(corpus)\n",
    "        stopwords = set(STOPWORDS)\n",
    "        stopwords.update([\"will\", \"report\", \"reporting\", \"market\", \"stock\", \"share\"])\n",
    "\n",
    "        wordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=100, background_color=\"white\", collocations=False).generate(text)\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        # Show most frequent words in a bar graph\n",
    "        most = counts.most_common()[:80]\n",
    "        x, y = [], []\n",
    "        for word, count in most:\n",
    "            if word not in stopwords:\n",
    "                x.append(word)\n",
    "                y.append(count)\n",
    "        plt.figure(figsize=(12,10))\n",
    "        sns.barplot(x=y, y=x)\n",
    "        plt.show()\n",
    "\n",
    "    return vocab,id2vocab,corpus,counts,bow\n",
    "\n",
    "messages = list(dataA.Text)\n",
    "vocab,id2vocab,corpus,counts,bow= create_vocab(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ac04ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff=set(Text)-set(words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ade3e1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All words: 95426\n",
      "Unique words: 11050\n",
      "Top 30 frequent words: ['.', ',', '<user>', '<url>', \"'\", '<number>', '<hashtag>', '</hashtag>', '<repeated>', '!', 'may', '-', ':', 'tomorrow', '?', '\"', 'th', 'day', '1', '<date>', 'going', 'st', 'apple', '&', '2', 'see', 'like', 'friday', 'amazon', 'time']\n"
     ]
    }
   ],
   "source": [
    "print(\"All words: {}\".format(len(words)))\n",
    "# Create Counter\n",
    "counts = Counter(words)\n",
    "print(\"Unique words: {}\".format(len(counts)))\n",
    "# Create BoW\n",
    "bow = sorted(counts, key=counts.get, reverse=True)\n",
    "print(\"Top 30 frequent words: {}\".format(bow[:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e51b0eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a312ba47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
