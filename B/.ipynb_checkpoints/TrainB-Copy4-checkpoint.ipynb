{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d60291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load main.py\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import nltk\n",
    "import ekphrasis\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1415c",
   "metadata": {},
   "source": [
    "### Step1: Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f090b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "Path =os.path.dirname(os.getcwd())\n",
    "data_pathB=os.path.join(Path,'Datasets/B/twitter-2016train-BD.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a8db65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>681563394940473347</td>\n",
       "      <td>amy schumer</td>\n",
       "      <td>negative</td>\n",
       "      <td>@MargaretsBelly Amy Schumer is the stereotypic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>675847244747177984</td>\n",
       "      <td>amy schumer</td>\n",
       "      <td>negative</td>\n",
       "      <td>@dani_pitter I mean I get the hype around JLaw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>672827854279843840</td>\n",
       "      <td>amy schumer</td>\n",
       "      <td>negative</td>\n",
       "      <td>Amy Schumer at the #GQmenoftheyear2015 party i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>662755012129529858</td>\n",
       "      <td>amy schumer</td>\n",
       "      <td>negative</td>\n",
       "      <td>Amy Schumer is on Sky Atlantic doing one of th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>671502639671042048</td>\n",
       "      <td>amy schumer</td>\n",
       "      <td>negative</td>\n",
       "      <td>Amy Schumer may have brought us Trainwreck, bu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10546</th>\n",
       "      <td>638032969383309312</td>\n",
       "      <td>zayn</td>\n",
       "      <td>positive</td>\n",
       "      <td>tomorrow I've to wake up  early so Zayn's erfo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10547</th>\n",
       "      <td>634711870570500096</td>\n",
       "      <td>zayn</td>\n",
       "      <td>positive</td>\n",
       "      <td>with Zayn gone I can now definitively say that...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10548</th>\n",
       "      <td>637134671797690368</td>\n",
       "      <td>zayn</td>\n",
       "      <td>positive</td>\n",
       "      <td>yo don't ever say that! god forbid! may it not...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10549</th>\n",
       "      <td>636413565780557824</td>\n",
       "      <td>zayn</td>\n",
       "      <td>positive</td>\n",
       "      <td>you may call me a bad fan but I sobbed so hard...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10550</th>\n",
       "      <td>634633336124776448</td>\n",
       "      <td>zayn</td>\n",
       "      <td>positive</td>\n",
       "      <td>zayn's voice: c'mon guys you can do it, nobody...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10551 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ID        Topic Sentiment  \\\n",
       "0      681563394940473347  amy schumer  negative   \n",
       "1      675847244747177984  amy schumer  negative   \n",
       "2      672827854279843840  amy schumer  negative   \n",
       "3      662755012129529858  amy schumer  negative   \n",
       "4      671502639671042048  amy schumer  negative   \n",
       "...                   ...          ...       ...   \n",
       "10546  638032969383309312         zayn  positive   \n",
       "10547  634711870570500096         zayn  positive   \n",
       "10548  637134671797690368         zayn  positive   \n",
       "10549  636413565780557824         zayn  positive   \n",
       "10550  634633336124776448         zayn  positive   \n",
       "\n",
       "                                                    Text  label  \n",
       "0      @MargaretsBelly Amy Schumer is the stereotypic...      0  \n",
       "1      @dani_pitter I mean I get the hype around JLaw...      0  \n",
       "2      Amy Schumer at the #GQmenoftheyear2015 party i...      0  \n",
       "3      Amy Schumer is on Sky Atlantic doing one of th...      0  \n",
       "4      Amy Schumer may have brought us Trainwreck, bu...      0  \n",
       "...                                                  ...    ...  \n",
       "10546  tomorrow I've to wake up  early so Zayn's erfo...      1  \n",
       "10547  with Zayn gone I can now definitively say that...      1  \n",
       "10548  yo don't ever say that! god forbid! may it not...      1  \n",
       "10549  you may call me a bad fan but I sobbed so hard...      1  \n",
       "10550  zayn's voice: c'mon guys you can do it, nobody...      1  \n",
       "\n",
       "[10551 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform data into df form\n",
    "dataB = pd.read_table(data_pathB,sep='\\t',header=None)\n",
    "dataB.columns = ['ID','Topic','Sentiment','Text','label']\n",
    "def add_label(sentiment):\n",
    "    if sentiment == 'negative':\n",
    "        return 0\n",
    "    elif sentiment == 'positive':\n",
    "        return 1\n",
    "\n",
    "dataB['label'] = dataB.Sentiment.apply(add_label)\n",
    "dataB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d65b7d12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sentiment distribution of data\n",
    "distrib=dataB.loc[:,['Sentiment','label']].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6be3383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label Distribution: {('positive', 1): 8212, ('negative', 0): 2339} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('label Distribution:',distrib,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6899762d",
   "metadata": {},
   "source": [
    "### Prepocess Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c415f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib=dataB.loc[:,'Topic'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feecd0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Topics: 100\n"
     ]
    }
   ],
   "source": [
    "topic_counts=Counter(dataB.Topic)\n",
    "print(\"Unique Topics: {}\".format(len(topic_counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f18f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ranked = topic_counts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b723837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['amy schumer', 'ant-man', 'bad blood', 'bee gees', 'big brother', 'boko haram', 'briana', 'brock lesnar', 'caitlyn jenner', 'calibraska', 'carly fiorina', 'cate blanchett', 'charlie hebdo', 'chris evans', 'christians', 'chuck norris', 'curtis', 'dana white', 'dark souls', 'david bowie', 'david price', 'david wright', 'dean ambrose', 'dunkin', 'dustin johnson', 'ed sheeran', 'eid', 'floyd mayweather', 'foo fighters', 'frank gifford', 'frank ocean', 'gay', 'george harrison', 'george osborne', 'gucci', 'hulk hogan', 'ice cube', 'ira', 'iran', 'iron maiden', 'islam', 'israel', 'janet jackson', 'jason aldean', 'john cena', 'john kasich', 'josh hamilton', 'justin bieber', 'kane', 'kanye west', 'katy perry', 'kendrick', 'kendrick lamar', 'kim kardashian', 'kpop', 'kris bryant', 'lady gaga', 'milan', 'miss usa', 'moto g', 'murray', 'muslims', 'naruto', 'national hot dog day', 'national ice cream day', 'niall', 'nicki', 'nirvana', 'paper towns', 'paul dunne', 'paul mccartney', 'prince george', 'ps4', 'rahul gandhi', 'randy orton', 'real madrid', 'red sox', 'rolling stone', 'rousey', 'ryan braun', 'sam smith', 'saudi arabia', 'scott walker', 'seth rollins', 'sharknado', 'shawn', 'star wars day', 'super eagles', 'the vamps', 'thor', 'tom brady', 'tony blair', 'twilight', 'u2', 'watchman', 'white sox', 'yakub', 'yoga', 'zac brown band', 'zayn'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7f6794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_topic(topic,text):\n",
    "    New_text=[]\n",
    "    for i in range(len(text)):\n",
    "        New_text.append(topic[i]+' '+text[i])\n",
    "    \n",
    "    return New_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c8e9c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_Topic = add_topic(dataB.Topic,dataB.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d3d25fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataB['Text_Topic']=Text_Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f202e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "      <th>Text_Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>681563394940473347</td>\n",
       "      <td>amy schumer</td>\n",
       "      <td>negative</td>\n",
       "      <td>@MargaretsBelly Amy Schumer is the stereotypic...</td>\n",
       "      <td>0</td>\n",
       "      <td>amy schumer @MargaretsBelly Amy Schumer is the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>675847244747177984</td>\n",
       "      <td>amy schumer</td>\n",
       "      <td>negative</td>\n",
       "      <td>@dani_pitter I mean I get the hype around JLaw...</td>\n",
       "      <td>0</td>\n",
       "      <td>amy schumer @dani_pitter I mean I get the hype...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>672827854279843840</td>\n",
       "      <td>amy schumer</td>\n",
       "      <td>negative</td>\n",
       "      <td>Amy Schumer at the #GQmenoftheyear2015 party i...</td>\n",
       "      <td>0</td>\n",
       "      <td>amy schumer Amy Schumer at the #GQmenoftheyear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>662755012129529858</td>\n",
       "      <td>amy schumer</td>\n",
       "      <td>negative</td>\n",
       "      <td>Amy Schumer is on Sky Atlantic doing one of th...</td>\n",
       "      <td>0</td>\n",
       "      <td>amy schumer Amy Schumer is on Sky Atlantic doi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>671502639671042048</td>\n",
       "      <td>amy schumer</td>\n",
       "      <td>negative</td>\n",
       "      <td>Amy Schumer may have brought us Trainwreck, bu...</td>\n",
       "      <td>0</td>\n",
       "      <td>amy schumer Amy Schumer may have brought us Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10546</th>\n",
       "      <td>638032969383309312</td>\n",
       "      <td>zayn</td>\n",
       "      <td>positive</td>\n",
       "      <td>tomorrow I've to wake up  early so Zayn's erfo...</td>\n",
       "      <td>1</td>\n",
       "      <td>zayn tomorrow I've to wake up  early so Zayn's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10547</th>\n",
       "      <td>634711870570500096</td>\n",
       "      <td>zayn</td>\n",
       "      <td>positive</td>\n",
       "      <td>with Zayn gone I can now definitively say that...</td>\n",
       "      <td>1</td>\n",
       "      <td>zayn with Zayn gone I can now definitively say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10548</th>\n",
       "      <td>637134671797690368</td>\n",
       "      <td>zayn</td>\n",
       "      <td>positive</td>\n",
       "      <td>yo don't ever say that! god forbid! may it not...</td>\n",
       "      <td>1</td>\n",
       "      <td>zayn yo don't ever say that! god forbid! may i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10549</th>\n",
       "      <td>636413565780557824</td>\n",
       "      <td>zayn</td>\n",
       "      <td>positive</td>\n",
       "      <td>you may call me a bad fan but I sobbed so hard...</td>\n",
       "      <td>1</td>\n",
       "      <td>zayn you may call me a bad fan but I sobbed so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10550</th>\n",
       "      <td>634633336124776448</td>\n",
       "      <td>zayn</td>\n",
       "      <td>positive</td>\n",
       "      <td>zayn's voice: c'mon guys you can do it, nobody...</td>\n",
       "      <td>1</td>\n",
       "      <td>zayn zayn's voice: c'mon guys you can do it, n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10551 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ID        Topic Sentiment  \\\n",
       "0      681563394940473347  amy schumer  negative   \n",
       "1      675847244747177984  amy schumer  negative   \n",
       "2      672827854279843840  amy schumer  negative   \n",
       "3      662755012129529858  amy schumer  negative   \n",
       "4      671502639671042048  amy schumer  negative   \n",
       "...                   ...          ...       ...   \n",
       "10546  638032969383309312         zayn  positive   \n",
       "10547  634711870570500096         zayn  positive   \n",
       "10548  637134671797690368         zayn  positive   \n",
       "10549  636413565780557824         zayn  positive   \n",
       "10550  634633336124776448         zayn  positive   \n",
       "\n",
       "                                                    Text  label  \\\n",
       "0      @MargaretsBelly Amy Schumer is the stereotypic...      0   \n",
       "1      @dani_pitter I mean I get the hype around JLaw...      0   \n",
       "2      Amy Schumer at the #GQmenoftheyear2015 party i...      0   \n",
       "3      Amy Schumer is on Sky Atlantic doing one of th...      0   \n",
       "4      Amy Schumer may have brought us Trainwreck, bu...      0   \n",
       "...                                                  ...    ...   \n",
       "10546  tomorrow I've to wake up  early so Zayn's erfo...      1   \n",
       "10547  with Zayn gone I can now definitively say that...      1   \n",
       "10548  yo don't ever say that! god forbid! may it not...      1   \n",
       "10549  you may call me a bad fan but I sobbed so hard...      1   \n",
       "10550  zayn's voice: c'mon guys you can do it, nobody...      1   \n",
       "\n",
       "                                              Text_Topic  \n",
       "0      amy schumer @MargaretsBelly Amy Schumer is the...  \n",
       "1      amy schumer @dani_pitter I mean I get the hype...  \n",
       "2      amy schumer Amy Schumer at the #GQmenoftheyear...  \n",
       "3      amy schumer Amy Schumer is on Sky Atlantic doi...  \n",
       "4      amy schumer Amy Schumer may have brought us Tr...  \n",
       "...                                                  ...  \n",
       "10546  zayn tomorrow I've to wake up  early so Zayn's...  \n",
       "10547  zayn with Zayn gone I can now definitively say...  \n",
       "10548  zayn yo don't ever say that! god forbid! may i...  \n",
       "10549  zayn you may call me a bad fan but I sobbed so...  \n",
       "10550  zayn zayn's voice: c'mon guys you can do it, n...  \n",
       "\n",
       "[10551 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443edeb7",
   "metadata": {},
   "source": [
    "1Case conversion\n",
    "包含“India”和“india”的语料库如果不应用小写化，机器会把它们识别为两个独立的术语，而实际上它们都是同一个单词的不同形式，并且对应于同一个国家。小写化后，仅存在一种“India”实例，即“india”，简化了在语料库中找到所有提到印度时的任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24103376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCY\\AppData\\Roaming\\Python\\Python38\\site-packages\\ekphrasis\\classes\\tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCY\\AppData\\Roaming\\Python\\Python38\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "#import ekphrasis library\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcbb4e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "# stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b296cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(Texts):\n",
    "    token=[]\n",
    "    for Text in Texts:\n",
    "        words = [sentence for sentence in text_processor.pre_process_doc(Text) if (sentence!='s' and sentence!='\\'')]\n",
    "#         words = [word for word in words if (word not in stop)]\n",
    "        token.append(words)\n",
    "    words=[word for words in token for word in words]\n",
    "    \n",
    "    print(\"All words: {}\".format(len(words)))\n",
    "    # Create Counter\n",
    "    counts = Counter(words)\n",
    "    print(\"Unique words: {}\".format(len(counts)))\n",
    "\n",
    "    Most_common= counts.most_common()[:30]\n",
    "    print(\"Top 30 most common words: {}\\n\".format(Most_common))\n",
    "    \n",
    "    vocab = {word: num for num, word in enumerate(counts, 1)}\n",
    "    return token,vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a39875da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All words: 270897\n",
      "Unique words: 14251\n",
      "Top 30 most common words: [('.', 9717), ('the', 8674), ('to', 4925), (',', 4803), ('i', 4709), ('<user>', 3385), ('a', 3298), ('and', 3226), ('!', 3152), ('in', 2985), ('<url>', 2814), ('is', 2712), ('of', 2673), ('on', 2633), ('<repeated>', 2578), ('<hashtag>', 2537), ('</hashtag>', 2537), ('<number>', 2336), ('it', 2291), ('for', 2245), ('<allcaps>', 2193), ('</allcaps>', 2193), ('you', 2183), ('be', 1883), ('may', 1849), ('tomorrow', 1819), ('-', 1794), ('not', 1771), ('with', 1726), ('day', 1581)]\n",
      "\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "token,vocab=Tokenize(dataB.Text_Topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ca1c7",
   "metadata": {},
   "source": [
    "### Step2: Word2Vec Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e51b0eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk import word_tokenize\n",
    "import multiprocessing\n",
    "import tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcf25f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model=Word2Vec(token,window=5, min_count=1,workers = multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b4c56bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20734075, 27089700)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.train(token, total_examples = len(token), epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b864413b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is summary of Word2Vec: Word2Vec(vocab=14251, vector_size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print('This is summary of Word2Vec: {}'.format(word2vec_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f349892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=word2vec_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ada745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.save_word2vec_format('Word2Vec.vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f820ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_matrix = np.zeros((len(index), 100))\n",
    "embed_dict={}\n",
    "for word, i in index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embed_matrix[i] = word2vec_model.wv[word]\n",
    "        embed_dict[word] = word2vec_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c0514fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del word2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f5bb152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word = pd.read_table(word_path,sep=' ',header=None)\n",
    "# word.set_index(0,inplace=True)\n",
    "# Embed_dict={}\n",
    "# for i in range(word.shape[0]):\n",
    "#     Embed_dict[word.index[i]]=word.iloc[i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eed71ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed_path=os.path.join(Path,'Datasets/Embed_dict')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ded29",
   "metadata": {},
   "source": [
    "### Step3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4e8d7c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCY\\AppData\\Roaming\\Python\\Python38\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.6.0\n",
      "Eager mode:  True\n",
      "GPU is available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCY\\AppData\\Local\\Temp\\ipykernel_29280\\281481831.py:23: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  display.set_matplotlib_formats('svg')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, SpatialDropout1D, Bidirectional\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import keras\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import transformers\n",
    "import tensorflow_hub as hub\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "display.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32cc1d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_lstm(X, vocab, seq_len):\n",
    "    '''\n",
    "    Returns tokenized tensor with padding in length 100\n",
    "    '''\n",
    "    X_tmp = np.zeros((len(X), seq_len), dtype=np.int64)\n",
    "    for i, text in enumerate(X):\n",
    "        tokens = [word for word in text_processor.pre_process_doc(text) if (word!='s' and word!='\\'')]\n",
    "#         tokens = [word for word in tokens if (word not in stop)]\n",
    "        token_ids = [vocab[word] for word in tokens if word in embed_dict.keys()]###\n",
    "        end_idx = min(len(token_ids), seq_len)\n",
    "        start_idx = max(seq_len - len(token_ids), 0)\n",
    "        X_tmp[i,start_idx:] = token_ids[:end_idx]\n",
    "\n",
    "    return X_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "74d4ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tokenizer_lstm(dataB.Text_Topic, vocab, 100)###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "53a0b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.one_hot(dataB.label, depth=2)\n",
    "Y= np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "93237066",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,Y_train, Y_test = train_test_split (X, Y, test_size=0.1, random_state=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "823b8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_layer(vocab, embed_dict):\n",
    "    \"\"\"\n",
    "    Build embedding matrix and embedding layer\n",
    "    :param vocab_size: vocabulary size\n",
    "    :param tok: tokenizer\n",
    "    :param embed_dict: embedding index\n",
    "    :return: embedding matrix and embedding layer\n",
    "    \"\"\"\n",
    "    #Build embedding matrix\n",
    "    vocab_size=len(vocab)+1\n",
    "    embedding_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, i in vocab.items():\n",
    "        # Vector corresponds to word\n",
    "        embedding_vector = embed_dict.get(word)###,embed_dict['<unk>']\n",
    "\n",
    "        if embedding_vector is not None:\n",
    "            # Ensure vector of embedding_matrix row matches word index\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    # Build embedding layer\n",
    "    embedding_layer = Embedding(input_dim = vocab_size, output_dim = 100, weights = [embedding_matrix], input_length = 100, trainable=False)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23ab1a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer=build_embedding_layer(vocab, embed_dict) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92d2e103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(X_train, y_train, embedding_layer):\n",
    "        \"\"\"\n",
    "        Train, validate and test BiLSTM model, calculate accuracy of training and validation set\n",
    "        :param X_train: tweet train data\n",
    "        :param y_train: sentiment label train data\n",
    "        :param embedding_layer: embedding layer\n",
    "        :param X_test: tweet test data\n",
    "        :param y_test: sentiment label test data\n",
    "        :return: accuracy, recall, precision, F1 score and history\n",
    "        \"\"\"\n",
    "        tf.debugging.set_log_device_placement(True)\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        model.add(SpatialDropout1D(0.2))\n",
    "        \n",
    "#         LSTM(128, dropout = 0.2, recurrent_dropout = 0.5)\n",
    "\n",
    "#         LSTM(128,activation='tanh', recurrent_activation='sigmoid',\n",
    "#              use_bias=True,dropout=0.5,recurrent_dropout=0.0)\n",
    "    \n",
    "#         model.add(Bidirectional(LSTM(128,dropout = 0.5,return_sequences=True)))\n",
    "#         model.add(Bidirectional(LSTM(64,dropout = 0.5)))   27  loss: 0.7183 - accuracy: 0.6815 - val_loss: 0.7699 - val_accuracy: 0.6634\n",
    "\n",
    "\n",
    "#         model.add(Bidirectional(LSTM(128,dropout = 0.5))) 26 loss: 0.7517 - accuracy: 0.6596 - val_loss: 0.7827 - val_accuracy: 0.6532\n",
    "        \n",
    "#         model.add(Bidirectional(LSTM(128,dropout = 0.5,return_sequences=True)))\n",
    "#         model.add(Bidirectional(LSTM(128,dropout = 0.0)))   28 loss: 0.6897 - accuracy: 0.6925 - val_loss: 0.7774 - val_accuracy: 0.6610\n",
    "\n",
    "#         model.add(Bidirectional(LSTM(128,dropout = 0.5,return_sequences=True)))\n",
    "#         model.add(Bidirectional(LSTM(64,dropout = 0.5)))  29 loss: 0.7080 - accuracy: 0.6759 - val_loss: 0.7618 - val_accuracy: 0.6653\n",
    "\n",
    "        model.add(Bidirectional(LSTM(128,dropout = 0.5,return_sequences=True)))\n",
    "        model.add(Bidirectional(LSTM(64,dropout = 0.5)))  #27 loss: 0.6735 - accuracy: 0.7028 - val_loss: 0.7640 - val_accuracy: 0.6669\n",
    "\n",
    "#         model.add(Bidirectional(LSTM(128,dropout = 0.3,recurrent_dropout = 0.5)))\n",
    "    \n",
    "        model.add(Dense(2, activation = 'softmax'))\n",
    "        model.summary()\n",
    "        \n",
    "        batch_size = 128\n",
    "        epochs = 50\n",
    "\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "                      metrics = ['Recall','Accuracy','Precision'])\n",
    "        history = model.fit(X_train, y_train, validation_split = 0.2, epochs = epochs, batch_size = batch_size)\n",
    "        model.save('taskB.h5'.format(23))\n",
    "        train_acc = history.history['Accuracy'][-1]\n",
    "        val_acc = history.history['val_Accuracy'][-1]\n",
    "        \n",
    "        train_rec = history.history['recall'][-1]\n",
    "        val_rec = history.history['val_recall'][-1]\n",
    "        \n",
    "        train_pre = history.history['precision'][-1]\n",
    "        val_pre = history.history['val_precision'][-1]\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "850fa6a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          1425200   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 256)          234496    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               164352    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 1,824,435\n",
      "Trainable params: 399,235\n",
      "Non-trainable params: 1,425,200\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "60/60 [==============================] - 8s 54ms/step - loss: 0.5769 - recall: 0.7385 - Accuracy: 0.7699 - precision: 0.7768 - val_loss: 0.4968 - val_recall: 0.8036 - val_Accuracy: 0.8036 - val_precision: 0.8036\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 2s 36ms/step - loss: 0.5319 - recall: 0.7771 - Accuracy: 0.7771 - precision: 0.7771 - val_loss: 0.5221 - val_recall: 0.8036 - val_Accuracy: 0.8036 - val_precision: 0.8036\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 2s 38ms/step - loss: 0.5353 - recall: 0.7771 - Accuracy: 0.7771 - precision: 0.7771 - val_loss: 0.5007 - val_recall: 0.8036 - val_Accuracy: 0.8036 - val_precision: 0.8036\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 2s 36ms/step - loss: 0.5310 - recall: 0.7771 - Accuracy: 0.7771 - precision: 0.7771 - val_loss: 0.4995 - val_recall: 0.8036 - val_Accuracy: 0.8036 - val_precision: 0.8036\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.5319 - recall: 0.7771 - Accuracy: 0.7771 - precision: 0.7771 - val_loss: 0.5012 - val_recall: 0.8036 - val_Accuracy: 0.8036 - val_precision: 0.8036\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.5309 - recall: 0.7773 - Accuracy: 0.7773 - precision: 0.7773 - val_loss: 0.5045 - val_recall: 0.8036 - val_Accuracy: 0.8036 - val_precision: 0.8036\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.5293 - recall: 0.7771 - Accuracy: 0.7771 - precision: 0.7771 - val_loss: 0.5014 - val_recall: 0.8036 - val_Accuracy: 0.8036 - val_precision: 0.8036\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 2s 34ms/step - loss: 0.5269 - recall: 0.7773 - Accuracy: 0.7773 - precision: 0.7773 - val_loss: 0.5054 - val_recall: 0.8031 - val_Accuracy: 0.8031 - val_precision: 0.8035\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.5255 - recall: 0.7775 - Accuracy: 0.7775 - precision: 0.7776 - val_loss: 0.5030 - val_recall: 0.8036 - val_Accuracy: 0.8036 - val_precision: 0.8036\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 2s 33ms/step - loss: 0.5235 - recall: 0.7771 - Accuracy: 0.7771 - precision: 0.7771 - val_loss: 0.5068 - val_recall: 0.8020 - val_Accuracy: 0.8020 - val_precision: 0.8020\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.5231 - recall: 0.7780 - Accuracy: 0.7780 - precision: 0.7781 - val_loss: 0.5100 - val_recall: 0.8015 - val_Accuracy: 0.8015 - val_precision: 0.8015\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 2s 34ms/step - loss: 0.5221 - recall: 0.7786 - Accuracy: 0.7786 - precision: 0.7786 - val_loss: 0.5092 - val_recall: 0.8020 - val_Accuracy: 0.8020 - val_precision: 0.8020\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 2s 37ms/step - loss: 0.5191 - recall: 0.7786 - Accuracy: 0.7786 - precision: 0.7786 - val_loss: 0.5217 - val_recall: 0.7962 - val_Accuracy: 0.7962 - val_precision: 0.7966\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 2s 36ms/step - loss: 0.5167 - recall: 0.7791 - Accuracy: 0.7791 - precision: 0.7791 - val_loss: 0.5207 - val_recall: 0.7941 - val_Accuracy: 0.7941 - val_precision: 0.7941\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 2s 34ms/step - loss: 0.5148 - recall: 0.7788 - Accuracy: 0.7788 - precision: 0.7789 - val_loss: 0.5231 - val_recall: 0.7957 - val_Accuracy: 0.7957 - val_precision: 0.7957\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 2s 33ms/step - loss: 0.5133 - recall: 0.7803 - Accuracy: 0.7803 - precision: 0.7803 - val_loss: 0.5469 - val_recall: 0.7862 - val_Accuracy: 0.7862 - val_precision: 0.7866\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.5092 - recall: 0.7801 - Accuracy: 0.7801 - precision: 0.7803 - val_loss: 0.5276 - val_recall: 0.7952 - val_Accuracy: 0.7952 - val_precision: 0.7952\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.5132 - recall: 0.7798 - Accuracy: 0.7798 - precision: 0.7799 - val_loss: 0.5259 - val_recall: 0.7967 - val_Accuracy: 0.7967 - val_precision: 0.7967\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.5045 - recall: 0.7795 - Accuracy: 0.7795 - precision: 0.7795 - val_loss: 0.5701 - val_recall: 0.7483 - val_Accuracy: 0.7483 - val_precision: 0.7483\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.5064 - recall: 0.7811 - Accuracy: 0.7811 - precision: 0.7811 - val_loss: 0.5400 - val_recall: 0.7899 - val_Accuracy: 0.7899 - val_precision: 0.7899\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.5017 - recall: 0.7829 - Accuracy: 0.7829 - precision: 0.7830 - val_loss: 0.5429 - val_recall: 0.7815 - val_Accuracy: 0.7815 - val_precision: 0.7815\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 2s 33ms/step - loss: 0.5000 - recall: 0.7825 - Accuracy: 0.7825 - precision: 0.7825 - val_loss: 0.5403 - val_recall: 0.7820 - val_Accuracy: 0.7820 - val_precision: 0.7820\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 2s 34ms/step - loss: 0.4933 - recall: 0.7837 - Accuracy: 0.7837 - precision: 0.7837 - val_loss: 0.5499 - val_recall: 0.7909 - val_Accuracy: 0.7909 - val_precision: 0.7909\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 3s 44ms/step - loss: 0.4950 - recall: 0.7819 - Accuracy: 0.7820 - precision: 0.7820 - val_loss: 0.5560 - val_recall: 0.7904 - val_Accuracy: 0.7904 - val_precision: 0.7904\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 3s 45ms/step - loss: 0.4978 - recall: 0.7826 - Accuracy: 0.7826 - precision: 0.7826 - val_loss: 0.5567 - val_recall: 0.7688 - val_Accuracy: 0.7688 - val_precision: 0.7688\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 2s 36ms/step - loss: 0.4909 - recall: 0.7832 - Accuracy: 0.7832 - precision: 0.7832 - val_loss: 0.5474 - val_recall: 0.7762 - val_Accuracy: 0.7762 - val_precision: 0.7762\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 2s 34ms/step - loss: 0.4895 - recall: 0.7850 - Accuracy: 0.7850 - precision: 0.7850 - val_loss: 0.5592 - val_recall: 0.7683 - val_Accuracy: 0.7683 - val_precision: 0.7683\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 2s 35ms/step - loss: 0.4859 - recall: 0.7853 - Accuracy: 0.7853 - precision: 0.7854 - val_loss: 0.5721 - val_recall: 0.7583 - val_Accuracy: 0.7583 - val_precision: 0.7583\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.4814 - recall: 0.7852 - Accuracy: 0.7852 - precision: 0.7852 - val_loss: 0.5901 - val_recall: 0.7330 - val_Accuracy: 0.7330 - val_precision: 0.7330\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 2s 31ms/step - loss: 0.4723 - recall: 0.7888 - Accuracy: 0.7888 - precision: 0.7888 - val_loss: 0.6085 - val_recall: 0.7399 - val_Accuracy: 0.7399 - val_precision: 0.7399\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.4693 - recall: 0.7896 - Accuracy: 0.7896 - precision: 0.7897 - val_loss: 0.5909 - val_recall: 0.7367 - val_Accuracy: 0.7367 - val_precision: 0.7367\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.4782 - recall: 0.7848 - Accuracy: 0.7848 - precision: 0.7848 - val_loss: 0.5917 - val_recall: 0.7430 - val_Accuracy: 0.7430 - val_precision: 0.7430\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 2s 38ms/step - loss: 0.4772 - recall: 0.7871 - Accuracy: 0.7871 - precision: 0.7871 - val_loss: 0.5816 - val_recall: 0.7662 - val_Accuracy: 0.7662 - val_precision: 0.7662\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 2s 41ms/step - loss: 0.4682 - recall: 0.7850 - Accuracy: 0.7850 - precision: 0.7850 - val_loss: 0.5907 - val_recall: 0.7588 - val_Accuracy: 0.7588 - val_precision: 0.7588\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 2s 41ms/step - loss: 0.4655 - recall: 0.7915 - Accuracy: 0.7916 - precision: 0.7916 - val_loss: 0.6182 - val_recall: 0.7425 - val_Accuracy: 0.7425 - val_precision: 0.7425\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 2s 34ms/step - loss: 0.4639 - recall: 0.7927 - Accuracy: 0.7927 - precision: 0.7927 - val_loss: 0.5986 - val_recall: 0.7599 - val_Accuracy: 0.7599 - val_precision: 0.7599\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 2s 32ms/step - loss: 0.4659 - recall: 0.7875 - Accuracy: 0.7875 - precision: 0.7875 - val_loss: 0.6069 - val_recall: 0.7378 - val_Accuracy: 0.7378 - val_precision: 0.7378\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 2s 36ms/step - loss: 0.4589 - recall: 0.7974 - Accuracy: 0.7974 - precision: 0.7974 - val_loss: 0.6225 - val_recall: 0.7525 - val_Accuracy: 0.7525 - val_precision: 0.7525\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 2s 37ms/step - loss: 0.4591 - recall: 0.7913 - Accuracy: 0.7913 - precision: 0.7913 - val_loss: 0.6264 - val_recall: 0.7283 - val_Accuracy: 0.7283 - val_precision: 0.7283\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 2s 33ms/step - loss: 0.4520 - recall: 0.7920 - Accuracy: 0.7921 - precision: 0.7921 - val_loss: 0.6496 - val_recall: 0.7241 - val_Accuracy: 0.7241 - val_precision: 0.7241\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 2s 41ms/step - loss: 0.4498 - recall: 0.7969 - Accuracy: 0.7969 - precision: 0.7969 - val_loss: 0.6313 - val_recall: 0.7393 - val_Accuracy: 0.7393 - val_precision: 0.7393\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 3s 42ms/step - loss: 0.4494 - recall: 0.8011 - Accuracy: 0.8011 - precision: 0.8011 - val_loss: 0.6389 - val_recall: 0.7335 - val_Accuracy: 0.7335 - val_precision: 0.7335\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 2s 33ms/step - loss: 0.4451 - recall: 0.8009 - Accuracy: 0.8009 - precision: 0.8009 - val_loss: 0.6403 - val_recall: 0.7388 - val_Accuracy: 0.7388 - val_precision: 0.7388\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 2s 36ms/step - loss: 0.4447 - recall: 0.8002 - Accuracy: 0.8002 - precision: 0.8002 - val_loss: 0.6608 - val_recall: 0.7246 - val_Accuracy: 0.7246 - val_precision: 0.7246\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 2s 35ms/step - loss: 0.4347 - recall: 0.8028 - Accuracy: 0.8028 - precision: 0.8028 - val_loss: 0.6682 - val_recall: 0.7046 - val_Accuracy: 0.7046 - val_precision: 0.7046\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 2s 37ms/step - loss: 0.4399 - recall: 0.8034 - Accuracy: 0.8034 - precision: 0.8034 - val_loss: 0.6531 - val_recall: 0.7156 - val_Accuracy: 0.7156 - val_precision: 0.7156\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 2s 35ms/step - loss: 0.4418 - recall: 0.8058 - Accuracy: 0.8058 - precision: 0.8059 - val_loss: 0.6631 - val_recall: 0.7125 - val_Accuracy: 0.7125 - val_precision: 0.7125\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 3s 43ms/step - loss: 0.4378 - recall: 0.8021 - Accuracy: 0.8021 - precision: 0.8021 - val_loss: 0.6335 - val_recall: 0.7451 - val_Accuracy: 0.7451 - val_precision: 0.7451\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 2s 39ms/step - loss: 0.4353 - recall: 0.8054 - Accuracy: 0.8054 - precision: 0.8054 - val_loss: 0.6637 - val_recall: 0.7241 - val_Accuracy: 0.7241 - val_precision: 0.7241\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 2s 33ms/step - loss: 0.4317 - recall: 0.8071 - Accuracy: 0.8071 - precision: 0.8071 - val_loss: 0.6417 - val_recall: 0.7335 - val_Accuracy: 0.7335 - val_precision: 0.7335\n"
     ]
    }
   ],
   "source": [
    "train_acc, val_acc, train_rec, val_rec, train_pre, val_pre,history= model_train(X_train, Y_train, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "871fec91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 1s 12ms/step - loss: 0.7239 - recall: 0.7112 - Accuracy: 0.7112 - precision: 0.7112\n"
     ]
    }
   ],
   "source": [
    "class_names = ['0: Negative','1: Neutra;', '2: Positive']\n",
    "\n",
    "model = load_model('taskB.h5')\n",
    "Y_pred = model.predict(X_test)\n",
    "Metrcs = model.evaluate(X_test, Y_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ef5c4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7238546013832092, 'recall': 0.7111742496490479, 'Accuracy': 0.7111742496490479, 'precision': 0.7111742496490479}\n"
     ]
    }
   ],
   "source": [
    "print(Metrcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa69f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# Load the planets dataset and initialize the figure\n",
    "planets = sns.load_dataset(\"planets\")\n",
    "g = sns.JointGrid(data=planets, x=\"year\", y=\"distance\", marginal_ticks=True)\n",
    "\n",
    "# Set a log scaling on the y axis\n",
    "g.ax_joint.set(yscale=\"log\")\n",
    "\n",
    "# Create an inset legend for the histogram colorbar\n",
    "cax = g.figure.add_axes([.15, .55, .02, .2])\n",
    "\n",
    "# Add the joint and marginal histogram plots\n",
    "g.plot_joint(\n",
    "    sns.histplot, discrete=(True, False),\n",
    "    cmap=\"light:#03012d\", pmax=.8, cbar=True, cbar_ax=cax\n",
    ")\n",
    "g.plot_marginals(sns.histplot, element=\"step\", color=\"#03012d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0540b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, val_acc, train_rec, val_rec, train_pre, val_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "320e6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_acc=history.history['Accuracy']\n",
    "Valid_acc=history.history['val_Accuracy']\n",
    "\n",
    "Train_rec=history.history['recall']\n",
    "Valid_rec=history.history['val_recall']\n",
    "\n",
    "Train_pre=history.history['precision']\n",
    "Valid_pre=history.history['val_precision']\n",
    "\n",
    "epochs=['Epoch '+str(time) for time in range(1,50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f165c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.JointGrid(data=planets, x=\"Train Accuracy\", y=\"Epoch\", marginal_ticks=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ce37c303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Epoch 1',\n",
       " 'Epoch 2',\n",
       " 'Epoch 3',\n",
       " 'Epoch 4',\n",
       " 'Epoch 5',\n",
       " 'Epoch 6',\n",
       " 'Epoch 7',\n",
       " 'Epoch 8',\n",
       " 'Epoch 9',\n",
       " 'Epoch 10',\n",
       " 'Epoch 11',\n",
       " 'Epoch 12',\n",
       " 'Epoch 13',\n",
       " 'Epoch 14',\n",
       " 'Epoch 15',\n",
       " 'Epoch 16',\n",
       " 'Epoch 17',\n",
       " 'Epoch 18',\n",
       " 'Epoch 19',\n",
       " 'Epoch 20',\n",
       " 'Epoch 21',\n",
       " 'Epoch 22',\n",
       " 'Epoch 23',\n",
       " 'Epoch 24',\n",
       " 'Epoch 25',\n",
       " 'Epoch 26',\n",
       " 'Epoch 27',\n",
       " 'Epoch 28',\n",
       " 'Epoch 29',\n",
       " 'Epoch 30',\n",
       " 'Epoch 31',\n",
       " 'Epoch 32',\n",
       " 'Epoch 33',\n",
       " 'Epoch 34',\n",
       " 'Epoch 35',\n",
       " 'Epoch 36',\n",
       " 'Epoch 37',\n",
       " 'Epoch 38',\n",
       " 'Epoch 39',\n",
       " 'Epoch 40',\n",
       " 'Epoch 41',\n",
       " 'Epoch 42',\n",
       " 'Epoch 43',\n",
       " 'Epoch 44',\n",
       " 'Epoch 45',\n",
       " 'Epoch 46',\n",
       " 'Epoch 47',\n",
       " 'Epoch 48',\n",
       " 'Epoch 49']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e91bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
